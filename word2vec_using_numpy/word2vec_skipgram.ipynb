{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_skipgram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_snw9LdxQ6Q5"
      },
      "source": [
        "%load_ext autoreload \n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L9QUJXERzmI"
      },
      "source": [
        "import numpy as np\n",
        "import re \n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u83cdz_kSskr"
      },
      "source": [
        "# Data Prep \n",
        "ReInventing the wheel is usually an awesome way to learn something deeply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8D_ppgPSTxc"
      },
      "source": [
        "def tokenize(text):\n",
        "    # obtains tokens with atlest 1 alphabet \n",
        "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
        "    return pattern.findall(text.lower())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81u2I1dFUL0S",
        "outputId": "510224d1-12a9-4190-fa1f-0c3d05563dde"
      },
      "source": [
        "tokenize('I love goa')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'goa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3eFAOaNUO9B"
      },
      "source": [
        "def mapping(tokens):\n",
        "    word_to_id = dict()\n",
        "    id_to_word = dict()\n",
        "    \n",
        "    for i, token in enumerate(set(tokens)):\n",
        "        word_to_id[token] = i\n",
        "        id_to_word[i] = token\n",
        "\n",
        "    \n",
        "    return word_to_id, id_to_word "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_plZiwKVlPO",
        "outputId": "e396b996-6bee-4ef7-b52f-d4907d86eb4a"
      },
      "source": [
        "mapping(['i', 'love', 'goa'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'goa': 1, 'i': 0, 'love': 2}, {0: 'i', 1: 'goa', 2: 'love'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_0dS7CAVojc"
      },
      "source": [
        "def generating_training_data(tokens, word_to_id, window_size):\n",
        "    N = len(tokens)\n",
        "    X, Y = [], []\n",
        "    for i in range(N):\n",
        "        nbr_inds = list(range(max(0, i-window_size), i)) + \\\n",
        "                    list(range(i+1, min(N, i+window_size + 1)))\n",
        "        for j in nbr_inds:\n",
        "            X.append(word_to_id[tokens[i]])\n",
        "            Y.append(word_to_id[tokens[j]])\n",
        "        \n",
        "    X = np.array(X)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    Y = np.array(Y)\n",
        "    Y = np.expand_dims(Y, axis=0)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXeZScexWu5P",
        "outputId": "f7839b64-b04e-4912-f34f-f5c7a2caf0ea"
      },
      "source": [
        "generating_training_data(['i', 'love', 'goa'], {'goa': 2, 'i': 1, 'love': 0}, 2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1, 1, 0, 0, 2, 2]]), array([[0, 2, 1, 2, 1, 0]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Jc-lzSYF2H"
      },
      "source": [
        "doc = \"\"\"\n",
        "The other variant of Word2Vec model works on these same lines but with a slightly different approach. In this case, the input layer is not a one hot embedding vector of just one word but two-three words put together i.e. a sequence. Then the model is trained to predict the probability vector, same as in skip-gram model. Once the model converges, we get the weights of the hidden layer nuerons which are the required embeddings of a particular word. Since, we consider a sequence of inputs, it is called ‘continuous bag of words or CBOW’. This variant helps us predict the next word of the given input sequence with good accuracy.\n",
        "\"\"\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfq5MvL1YiKG"
      },
      "source": [
        "tokens = tokenize(doc)\n",
        "word_to_id, id_to_word = mapping(tokens)\n",
        "X, Y = generating_training_data(tokens, word_to_id, 3)\n",
        "vocab_size = len(id_to_word)\n",
        "m = Y.shape[1]\n",
        "# turn Y into one hot encoding \n",
        "Y_one_hot = np.zeros((vocab_size, m))\n",
        "Y_one_hot[Y.flatten(), np.arange(m)]=1"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nCNkt4LZMiV",
        "outputId": "d59f11d2-0286-40c6-90a2-a671ad3a5658"
      },
      "source": [
        "print(X, Y)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[68 68 68 10 10 10 10 71 71 71 71 71 11 11 11 11 11 11 57 57 57 57 57 57\n",
            "  35 35 35 35 35 35 20 20 20 20 20 20 23 23 23 23 23 23 27 27 27 27 27 27\n",
            "  30 30 30 30 30 30  7  7  7  7  7  7 55 55 55 55 55 55 40 40 40 40 40 40\n",
            "  19 19 19 19 19 19 56 56 56 56 56 56 65 65 65 65 65 65 51 51 51 51 51 51\n",
            "   8  8  8  8  8  8  0  0  0  0  0  0 46 46 46 46 46 46 68 68 68 68 68 68\n",
            "   5  5  5  5  5  5 58 58 58 58 58 58 15 15 15 15 15 15 25 25 25 25 25 25\n",
            "  19 19 19 19 19 19 36 36 36 36 36 36 69 69 69 69 69 69 31 31 31 31 31 31\n",
            "  32 32 32 32 32 32 11 11 11 11 11 11  1  1  1  1  1  1 36 36 36 36 36 36\n",
            "  48 48 48 48 48 48 55 55 55 55 55 55  2  2  2  2  2  2 72 72 72 72 72 72\n",
            "  33 33 33 33 33 33 45 45 45 45 45 45 63 63 63 63 63 63 34 34 34 34 34 34\n",
            "  52 52 52 52 52 52 19 19 19 19 19 19 37 37 37 37 37 37 70 70 70 70 70 70\n",
            "  68 68 68 68 68 68 35 35 35 35 35 35 15 15 15 15 15 15 28 28 28 28 28 28\n",
            "  67 67 67 67 67 67 24 24 24 24 24 24 68 68 68 68 68 68 49 49 49 49 49 49\n",
            "  32 32 32 32 32 32 30 30 30 30 30 30 64 64 64 64 64 64  8  8  8  8  8  8\n",
            "  41 41 41 41 41 41 50 50 50 50 50 50 35 35 35 35 35 35 21 21 21 21 21 21\n",
            "  68 68 68 68 68 68 35 35 35 35 35 35 18 18 18 18 18 18 42 42 42 42 42 42\n",
            "  22 22 22 22 22 22 68 68 68 68 68 68 59 59 59 59 59 59 11 11 11 11 11 11\n",
            "  68 68 68 68 68 68 38 38 38 38 38 38 58 58 58 58 58 58 14 14 14 14 14 14\n",
            "   4  4  4  4  4  4 54 54 54 54 54 54 68 68 68 68 68 68 60 60 60 60 60 60\n",
            "   9  9  9  9  9  9 11 11 11 11 11 11 19 19 19 19 19 19 61 61 61 61 61 61\n",
            "  48 48 48 48 48 48 66 66 66 66 66 66 42 42 42 42 42 42 29 29 29 29 29 29\n",
            "  19 19 19 19 19 19 37 37 37 37 37 37 11 11 11 11 11 11 53 53 53 53 53 53\n",
            "  16 16 16 16 16 16 15 15 15 15 15 15  3  3  3  3  3  3  6  6  6  6  6  6\n",
            "  47 47 47 47 47 47 11 11 11 11 11 11 33 33 33 33 33 33 26 26 26 26 26 26\n",
            "  12 12 12 12 12 12  0  0  0  0  0  0 71 71 71 71 71 71 43 43 43 43 43 43\n",
            "  39 39 39 39 39 39 24 24 24 24 24 24 68 68 68 68 68 68 17 17 17 17 17 17\n",
            "  48 48 48 48 48 48 11 11 11 11 11 11 68 68 68 68 68 68 44 44 44 44 44 44\n",
            "   5  5  5  5  5  5 37 37 37 37 37 37 40 40 40 40 40 13 13 13 13 62 62 62]] [[10 71 11 68 71 11 57 68 10 11 57 35 68 10 71 57 35 20 10 71 11 35 20 23\n",
            "  71 11 57 20 23 27 11 57 35 23 27 30 57 35 20 27 30  7 35 20 23 30  7 55\n",
            "  20 23 27  7 55 40 23 27 30 55 40 19 27 30  7 40 19 56 30  7 55 19 56 65\n",
            "   7 55 40 56 65 51 55 40 19 65 51  8 40 19 56 51  8  0 19 56 65  8  0 46\n",
            "  56 65 51  0 46 68 65 51  8 46 68  5 51  8  0 68  5 58  8  0 46  5 58 15\n",
            "   0 46 68 58 15 25 46 68  5 15 25 19 68  5 58 25 19 36  5 58 15 19 36 69\n",
            "  58 15 25 36 69 31 15 25 19 69 31 32 25 19 36 31 32 11 19 36 69 32 11  1\n",
            "  36 69 31 11  1 36 69 31 32  1 36 48 31 32 11 36 48 55 32 11  1 48 55  2\n",
            "  11  1 36 55  2 72  1 36 48  2 72 33 36 48 55 72 33 45 48 55  2 33 45 63\n",
            "  55  2 72 45 63 34  2 72 33 63 34 52 72 33 45 34 52 19 33 45 63 52 19 37\n",
            "  45 63 34 19 37 70 63 34 52 37 70 68 34 52 19 70 68 35 52 19 37 68 35 15\n",
            "  19 37 70 35 15 28 37 70 68 15 28 67 70 68 35 28 67 24 68 35 15 67 24 68\n",
            "  35 15 28 24 68 49 15 28 67 68 49 32 28 67 24 49 32 30 67 24 68 32 30 64\n",
            "  24 68 49 30 64  8 68 49 32 64  8 41 49 32 30  8 41 50 32 30 64 41 50 35\n",
            "  30 64  8 50 35 21 64  8 41 35 21 68  8 41 50 21 68 35 41 50 35 68 35 18\n",
            "  50 35 21 35 18 42 35 21 68 18 42 22 21 68 35 42 22 68 68 35 18 22 68 59\n",
            "  35 18 42 68 59 11 18 42 22 59 11 68 42 22 68 11 68 38 22 68 59 68 38 58\n",
            "  68 59 11 38 58 14 59 11 68 58 14  4 11 68 38 14  4 54 68 38 58  4 54 68\n",
            "  38 58 14 54 68 60 58 14  4 68 60  9 14  4 54 60  9 11  4 54 68  9 11 19\n",
            "  54 68 60 11 19 61 68 60  9 19 61 48 60  9 11 61 48 66  9 11 19 48 66 42\n",
            "  11 19 61 66 42 29 19 61 48 42 29 19 61 48 66 29 19 37 48 66 42 19 37 11\n",
            "  66 42 29 37 11 53 42 29 19 11 53 16 29 19 37 53 16 15 19 37 11 16 15  3\n",
            "  37 11 53 15  3  6 11 53 16  3  6 47 53 16 15  6 47 11 16 15  3 47 11 33\n",
            "  15  3  6 11 33 26  3  6 47 33 26 12  6 47 11 26 12  0 47 11 33 12  0 71\n",
            "  11 33 26  0 71 43 33 26 12 71 43 39 26 12  0 43 39 24 12  0 71 39 24 68\n",
            "   0 71 43 24 68 17 71 43 39 68 17 48 43 39 24 17 48 11 39 24 68 48 11 68\n",
            "  24 68 17 11 68 44 68 17 48 68 44  5 17 48 11 44  5 37 48 11 68  5 37 40\n",
            "  11 68 44 37 40 13 68 44  5 40 13 62 44  5 37 13 62  5 37 40 62 37 40 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9l_uzkZORL",
        "outputId": "2d77600a-425c-48cf-c24a-d4bef05103f2"
      },
      "source": [
        "print(Y_one_hot)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzkskEVcaVav"
      },
      "source": [
        "#Initialization \n",
        "* word Embedding \n",
        "* Dense Layer \n",
        "* Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM4csjGbaE-8"
      },
      "source": [
        "def initialize_wrd_emb(vocab_size, emb_size):\n",
        "    WRD_EMB = np.random.randn(vocab_size, emb_size)*0.01\n",
        "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
        "    return WRD_EMB"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npIjTK4MbBcl"
      },
      "source": [
        "def initialize_dense(input_size, output_size):\n",
        "    W = np.random.randn(output_size, input_size) * 0.01\n",
        "\n",
        "    assert(W.shape == (output_size, input_size))\n",
        "\n",
        "    return W"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9l5EsMbj9F"
      },
      "source": [
        "def initialize_parameters(vocab_size, emb_size):\n",
        "    WRD_EMB  = initialize_wrd_emb(vocab_size, emb_size)\n",
        "    W = initialize_dense(emb_size, vocab_size)\n",
        "\n",
        "    parameters = {}\n",
        "    parameters['WRD_EMB'] = WRD_EMB \n",
        "    parameters['W'] = W\n",
        "    return parameters"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbqD0ZwucMO-",
        "outputId": "d14b0eee-4c17-4585-cd8d-cae89e74f74d"
      },
      "source": [
        "print(initialize_parameters(5, 5))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'WRD_EMB': array([[ 0.00797424, -0.00102398, -0.01053327,  0.01144288,  0.01087524],\n",
            "       [-0.0014608 ,  0.00014197, -0.00152098, -0.00103424, -0.01377535],\n",
            "       [ 0.01346266, -0.00090859,  0.00369364,  0.01376817, -0.0156851 ],\n",
            "       [ 0.01112075,  0.01053788, -0.00350727, -0.00291178, -0.01632778],\n",
            "       [ 0.00185193,  0.00080155,  0.00548211,  0.01214297,  0.00353309]]), 'W': array([[ 0.0163293 , -0.00488816, -0.00443603, -0.01363806, -0.01073577],\n",
            "       [-0.01489897, -0.01363439, -0.01334275,  0.01253328, -0.00738896],\n",
            "       [ 0.00059801, -0.00407217, -0.01031124,  0.00515161,  0.00789267],\n",
            "       [ 0.00186501, -0.01324542,  0.0141502 ,  0.00133391, -0.01127588],\n",
            "       [-0.00423727, -0.01283284,  0.00081796, -0.01422961,  0.00207391]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48v-0yxB15Co"
      },
      "source": [
        "# FORWARD PROPAGATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23wNFnCecSby"
      },
      "source": [
        "def int_to_word_vecs(inds, parameters):\n",
        "    \"\"\"\n",
        "    inds : numpy array. shape:(1, m)\n",
        "    parameters : dict. weights to be trained\n",
        "    \"\"\"\n",
        "    m = inds.shape[1]\n",
        "    WRD_EMB = parameters['WRD_EMB']\n",
        "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
        "\n",
        "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "\n",
        "\n",
        "def linear_dense(word_vec, parameters):\n",
        "    \"\"\"\n",
        "    word_vec : numpy array. shape : (emb_size, m)\n",
        "    parameters : dict. weights to be trained\n",
        "    \"\"\"\n",
        "\n",
        "    m = word_vec.shape[1]\n",
        "    W = parameters['W']\n",
        "    Z = np.dot(W, word_vec)\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], m))\n",
        "\n",
        "    return W, Z"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDhzHBM3oHi"
      },
      "source": [
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Z : output out of the dense layer. shape : (vocab_size, m)\n",
        "    \"\"\"\n",
        "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
        "\n",
        "    assert(softmax_out.shape == Z.shape)\n",
        "\n",
        "    return softmax_out"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQvm96GZ4VWa"
      },
      "source": [
        "def forward_propagation(inds, parameters):\n",
        "    word_vec = int_to_word_vecs(inds, parameters)\n",
        "    W, Z = linear_dense(word_vec, parameters)\n",
        "    softmax_out = softmax(Z)\n",
        "\n",
        "    caches = {}\n",
        "    caches['inds'] = inds\n",
        "    caches['word_vec'] = word_vec\n",
        "    caches['W'] = W\n",
        "    caches['Z'] = Z\n",
        "\n",
        "    return softmax_out, caches"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s20n7zdA4_zD"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_-LURsa5Cfe"
      },
      "source": [
        "# COST FUNCTION (CROSS ENTROPY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwbF9s2I5GNL"
      },
      "source": [
        "def cross_entropy(softmax_out, Y):\n",
        "    \"\"\"\n",
        "    softmax_out output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "\n",
        "    m = softmax_out.shape[1]\n",
        "\n",
        "    cost = -(1/m)*np.sum(np.sum(Y* np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3qbPxUs592f"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v93Wxihw6BNB"
      },
      "source": [
        "# BACKWARD PROPAGATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1FeqWt56DKW"
      },
      "source": [
        "def softmax_backward(Y, softmax_out):\n",
        "    \"\"\"\n",
        "    Y: labels of training data. shape:(vocab_size, m)\n",
        "    softmax_out : output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "\n",
        "    dL_dZ = softmax_out - Y\n",
        "    assert(dL_dZ.shape == softmax_out.shape)\n",
        "\n",
        "    return dL_dZ"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McVdcLc16rZU"
      },
      "source": [
        "def dense_backward(dL_dZ, caches):\n",
        "    \"\"\"\n",
        "    dL_dZ : shape : (vocab_size, m)\n",
        "    caches: dict. results from each steps of forward propagation\n",
        "    \"\"\"\n",
        "    W = caches['W']\n",
        "    word_vec = caches['word_vec']\n",
        "    m = word_vec.shape[1]\n",
        "\n",
        "    dL_dW = (1/m) * np.dot(dL_dZ, word_vec.T)\n",
        "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
        "\n",
        "    assert( W.shape == dL_dW.shape)\n",
        "    assert( word_vec.shape == dL_dword_vec.shape)\n",
        "\n",
        "    return dL_dW, dL_dword_vec"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgxTY0wC8BD9"
      },
      "source": [
        "def backward_propagation(Y, softmax_out, caches):\n",
        "    dL_dZ = softmax_backward(Y, softmax_out)\n",
        "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
        "    \n",
        "    gradients = dict()\n",
        "    gradients['dL_dZ'] = dL_dZ\n",
        "    gradients['dL_dW'] = dL_dW\n",
        "    gradients['dL_dword_vec'] = dL_dword_vec\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, caches, gradients, learning_rate):\n",
        "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
        "    inds = caches['inds']\n",
        "    dL_dword_vec = gradients['dL_dword_vec']\n",
        "    m = inds.shape[-1]\n",
        "    \n",
        "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
        "\n",
        "    parameters['W'] -= learning_rate * gradients['dL_dW']"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS7t286q8PkB"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    if parameters is None:\n",
        "        parameters = initialize_parameters(vocab_size, emb_size)\n",
        "    \n",
        "    begin_time = datetime.now()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_cost = 0\n",
        "        batch_inds = list(range(0, m, batch_size))\n",
        "        np.random.shuffle(batch_inds)\n",
        "        for i in batch_inds:\n",
        "            X_batch = X[:, i:i+batch_size]\n",
        "            Y_batch = Y[:, i:i+batch_size]\n",
        "\n",
        "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
        "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
        "            update_parameters(parameters, caches, gradients, learning_rate)\n",
        "            cost = cross_entropy(softmax_out, Y_batch)\n",
        "            epoch_cost += np.squeeze(cost)\n",
        "            \n",
        "        costs.append(epoch_cost)\n",
        "        if print_cost and epoch % (epochs // 500) == 0:\n",
        "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
        "        if epoch % (epochs // 100) == 0:\n",
        "            learning_rate *= 0.98\n",
        "    end_time = datetime.now()\n",
        "    print('training time: {}'.format(end_time - begin_time))\n",
        "            \n",
        "    if plot_cost:\n",
        "        plt.plot(np.arange(epochs), costs)\n",
        "        plt.xlabel('# of epochs')\n",
        "        plt.ylabel('cost')\n",
        "    return parameters"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7y-9lnbt8TBh",
        "outputId": "031e4618-7a55-44d8-b971-e89670a31e44"
      },
      "source": [
        "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10000, batch_size=128, parameters=None, print_cost=True)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0: 25.32007458325938\n",
            "Cost after epoch 20: 25.314794542012667\n",
            "Cost after epoch 40: 25.305967518484003\n",
            "Cost after epoch 60: 25.288471452150716\n",
            "Cost after epoch 80: 25.253032832717036\n",
            "Cost after epoch 100: 25.182028493416098\n",
            "Cost after epoch 120: 25.046035775049923\n",
            "Cost after epoch 140: 24.793942036256833\n",
            "Cost after epoch 160: 24.382722432108462\n",
            "Cost after epoch 180: 23.88873674562499\n",
            "Cost after epoch 200: 23.457170469396797\n",
            "Cost after epoch 220: 23.111016580620525\n",
            "Cost after epoch 240: 22.835884540782672\n",
            "Cost after epoch 260: 22.5959394629104\n",
            "Cost after epoch 280: 22.358757233577148\n",
            "Cost after epoch 300: 22.103688627245624\n",
            "Cost after epoch 320: 21.848049771335283\n",
            "Cost after epoch 340: 21.5758281315375\n",
            "Cost after epoch 360: 21.286236498572706\n",
            "Cost after epoch 380: 20.992694874453008\n",
            "Cost after epoch 400: 20.698872206922054\n",
            "Cost after epoch 420: 20.42446124047133\n",
            "Cost after epoch 440: 20.166038022587493\n",
            "Cost after epoch 460: 19.929610912678207\n",
            "Cost after epoch 480: 19.7228169231509\n",
            "Cost after epoch 500: 19.533515012977404\n",
            "Cost after epoch 520: 19.370709614549256\n",
            "Cost after epoch 540: 19.225795814382774\n",
            "Cost after epoch 560: 19.098369466319575\n",
            "Cost after epoch 580: 18.99110432403485\n",
            "Cost after epoch 600: 18.89695013597628\n",
            "Cost after epoch 620: 18.818875819262303\n",
            "Cost after epoch 640: 18.751077726814678\n",
            "Cost after epoch 660: 18.695484254522096\n",
            "Cost after epoch 680: 18.65082481292128\n",
            "Cost after epoch 700: 18.60168602132138\n",
            "Cost after epoch 720: 18.56444238720729\n",
            "Cost after epoch 740: 18.535721846626398\n",
            "Cost after epoch 760: 18.51368571708899\n",
            "Cost after epoch 780: 18.482310361034624\n",
            "Cost after epoch 800: 18.473518350125275\n",
            "Cost after epoch 820: 18.442375743449322\n",
            "Cost after epoch 840: 18.436289894909113\n",
            "Cost after epoch 860: 18.410614734879605\n",
            "Cost after epoch 880: 18.40878127939982\n",
            "Cost after epoch 900: 18.406287327753315\n",
            "Cost after epoch 920: 18.382082410136594\n",
            "Cost after epoch 940: 18.38578133848182\n",
            "Cost after epoch 960: 18.379233276693768\n",
            "Cost after epoch 980: 18.376413910562494\n",
            "Cost after epoch 1000: 18.379134483373452\n",
            "Cost after epoch 1020: 18.36167937046896\n",
            "Cost after epoch 1040: 18.367955304372153\n",
            "Cost after epoch 1060: 18.36569244394041\n",
            "Cost after epoch 1080: 18.370666299881428\n",
            "Cost after epoch 1100: 18.358069872361554\n",
            "Cost after epoch 1120: 18.355571525944917\n",
            "Cost after epoch 1140: 18.371404651736686\n",
            "Cost after epoch 1160: 18.370180357670776\n",
            "Cost after epoch 1180: 18.374165717480683\n",
            "Cost after epoch 1200: 18.365121845737473\n",
            "Cost after epoch 1220: 18.36356847455986\n",
            "Cost after epoch 1240: 18.3635804740083\n",
            "Cost after epoch 1260: 18.361922569174375\n",
            "Cost after epoch 1280: 18.35241543900321\n",
            "Cost after epoch 1300: 18.350679958678043\n",
            "Cost after epoch 1320: 18.334256517866912\n",
            "Cost after epoch 1340: 18.332418829914367\n",
            "Cost after epoch 1360: 18.318988790408344\n",
            "Cost after epoch 1380: 18.315068450277334\n",
            "Cost after epoch 1400: 18.31028776537159\n",
            "Cost after epoch 1420: 18.31095509208007\n",
            "Cost after epoch 1440: 18.314996521296532\n",
            "Cost after epoch 1460: 18.30530441002952\n",
            "Cost after epoch 1480: 18.309009173927794\n",
            "Cost after epoch 1500: 18.31652320853035\n",
            "Cost after epoch 1520: 18.330613926113404\n",
            "Cost after epoch 1540: 18.317934807684182\n",
            "Cost after epoch 1560: 18.316609806117377\n",
            "Cost after epoch 1580: 18.318110844640138\n",
            "Cost after epoch 1600: 18.32345820054786\n",
            "Cost after epoch 1620: 18.31460853378687\n",
            "Cost after epoch 1640: 18.328849116244314\n",
            "Cost after epoch 1660: 18.34110260686754\n",
            "Cost after epoch 1680: 18.323721089567393\n",
            "Cost after epoch 1700: 18.334285440543255\n",
            "Cost after epoch 1720: 18.329149795415862\n",
            "Cost after epoch 1740: 18.316785490405945\n",
            "Cost after epoch 1760: 18.304042266689134\n",
            "Cost after epoch 1780: 18.31043785574932\n",
            "Cost after epoch 1800: 18.31703567779457\n",
            "Cost after epoch 1820: 18.295509374224267\n",
            "Cost after epoch 1840: 18.29056461676732\n",
            "Cost after epoch 1860: 18.281461434641855\n",
            "Cost after epoch 1880: 18.27362035856178\n",
            "Cost after epoch 1900: 18.285850745138017\n",
            "Cost after epoch 1920: 18.28578340729466\n",
            "Cost after epoch 1940: 18.27422997691393\n",
            "Cost after epoch 1960: 18.288743092151066\n",
            "Cost after epoch 1980: 18.292286839320123\n",
            "Cost after epoch 2000: 18.278007924821008\n",
            "Cost after epoch 2020: 18.284247702373456\n",
            "Cost after epoch 2040: 18.278326602826986\n",
            "Cost after epoch 2060: 18.300429219737662\n",
            "Cost after epoch 2080: 18.29892608804366\n",
            "Cost after epoch 2100: 18.305023875421437\n",
            "Cost after epoch 2120: 18.29664274754991\n",
            "Cost after epoch 2140: 18.30032725611266\n",
            "Cost after epoch 2160: 18.29786758142319\n",
            "Cost after epoch 2180: 18.27560240503575\n",
            "Cost after epoch 2200: 18.284995850772905\n",
            "Cost after epoch 2220: 18.265897338568518\n",
            "Cost after epoch 2240: 18.266138564408692\n",
            "Cost after epoch 2260: 18.28427748488375\n",
            "Cost after epoch 2280: 18.291215340590476\n",
            "Cost after epoch 2300: 18.284941287105923\n",
            "Cost after epoch 2320: 18.298093801192106\n",
            "Cost after epoch 2340: 18.29351533707337\n",
            "Cost after epoch 2360: 18.315562549562276\n",
            "Cost after epoch 2380: 18.311567586008586\n",
            "Cost after epoch 2400: 18.338670809961105\n",
            "Cost after epoch 2420: 18.319738365337397\n",
            "Cost after epoch 2440: 18.337995738370523\n",
            "Cost after epoch 2460: 18.370997281866753\n",
            "Cost after epoch 2480: 18.369929800695708\n",
            "Cost after epoch 2500: 18.387151614032273\n",
            "Cost after epoch 2520: 18.41147290454588\n",
            "Cost after epoch 2540: 18.401599777159696\n",
            "Cost after epoch 2560: 18.41444044343425\n",
            "Cost after epoch 2580: 18.407589402733453\n",
            "Cost after epoch 2600: 18.45017746189161\n",
            "Cost after epoch 2620: 18.442223485125915\n",
            "Cost after epoch 2640: 18.434276913815786\n",
            "Cost after epoch 2660: 18.420510465466386\n",
            "Cost after epoch 2680: 18.42114471546194\n",
            "Cost after epoch 2700: 18.44185795626429\n",
            "Cost after epoch 2720: 18.423726649315412\n",
            "Cost after epoch 2740: 18.44094224318649\n",
            "Cost after epoch 2760: 18.425492738303095\n",
            "Cost after epoch 2780: 18.430008984170545\n",
            "Cost after epoch 2800: 18.402107825779115\n",
            "Cost after epoch 2820: 18.405873579127086\n",
            "Cost after epoch 2840: 18.387824176254526\n",
            "Cost after epoch 2860: 18.39401055224927\n",
            "Cost after epoch 2880: 18.402446371356607\n",
            "Cost after epoch 2900: 18.399945628646297\n",
            "Cost after epoch 2920: 18.411014116232458\n",
            "Cost after epoch 2940: 18.405368907757573\n",
            "Cost after epoch 2960: 18.3751416347757\n",
            "Cost after epoch 2980: 18.383146786301854\n",
            "Cost after epoch 3000: 18.40891696339924\n",
            "Cost after epoch 3020: 18.400106755516795\n",
            "Cost after epoch 3040: 18.3721197315346\n",
            "Cost after epoch 3060: 18.38568861106888\n",
            "Cost after epoch 3080: 18.39866605178613\n",
            "Cost after epoch 3100: 18.405988211929703\n",
            "Cost after epoch 3120: 18.407360696205053\n",
            "Cost after epoch 3140: 18.3888779536775\n",
            "Cost after epoch 3160: 18.393554627223608\n",
            "Cost after epoch 3180: 18.38171009372357\n",
            "Cost after epoch 3200: 18.36220032476945\n",
            "Cost after epoch 3220: 18.354486878839907\n",
            "Cost after epoch 3240: 18.344059112356533\n",
            "Cost after epoch 3260: 18.35993228686285\n",
            "Cost after epoch 3280: 18.354004129453713\n",
            "Cost after epoch 3300: 18.35929220588563\n",
            "Cost after epoch 3320: 18.331646499835813\n",
            "Cost after epoch 3340: 18.32707902674582\n",
            "Cost after epoch 3360: 18.335226031628817\n",
            "Cost after epoch 3380: 18.329071353696467\n",
            "Cost after epoch 3400: 18.315243473719388\n",
            "Cost after epoch 3420: 18.291879521806138\n",
            "Cost after epoch 3440: 18.323194084065943\n",
            "Cost after epoch 3460: 18.30938393166337\n",
            "Cost after epoch 3480: 18.316952220304\n",
            "Cost after epoch 3500: 18.311307569353307\n",
            "Cost after epoch 3520: 18.28405413419853\n",
            "Cost after epoch 3540: 18.294487814493472\n",
            "Cost after epoch 3560: 18.321488564878436\n",
            "Cost after epoch 3580: 18.31428777206599\n",
            "Cost after epoch 3600: 18.318768528768416\n",
            "Cost after epoch 3620: 18.304598336359255\n",
            "Cost after epoch 3640: 18.319923835748025\n",
            "Cost after epoch 3660: 18.312493964590175\n",
            "Cost after epoch 3680: 18.324548838935087\n",
            "Cost after epoch 3700: 18.314549158313127\n",
            "Cost after epoch 3720: 18.340451158122303\n",
            "Cost after epoch 3740: 18.31462212511635\n",
            "Cost after epoch 3760: 18.314694374512698\n",
            "Cost after epoch 3780: 18.35643361612296\n",
            "Cost after epoch 3800: 18.346791863256907\n",
            "Cost after epoch 3820: 18.32370102477949\n",
            "Cost after epoch 3840: 18.330632516667094\n",
            "Cost after epoch 3860: 18.343438832549246\n",
            "Cost after epoch 3880: 18.32596272617902\n",
            "Cost after epoch 3900: 18.345101712300938\n",
            "Cost after epoch 3920: 18.345072606803313\n",
            "Cost after epoch 3940: 18.336545946311897\n",
            "Cost after epoch 3960: 18.336172614328298\n",
            "Cost after epoch 3980: 18.32876935286295\n",
            "Cost after epoch 4000: 18.33531096466079\n",
            "Cost after epoch 4020: 18.31479031640972\n",
            "Cost after epoch 4040: 18.334171182334906\n",
            "Cost after epoch 4060: 18.332904386498285\n",
            "Cost after epoch 4080: 18.31060492215996\n",
            "Cost after epoch 4100: 18.343810423553613\n",
            "Cost after epoch 4120: 18.32828566553799\n",
            "Cost after epoch 4140: 18.322584275860866\n",
            "Cost after epoch 4160: 18.337686429243032\n",
            "Cost after epoch 4180: 18.37257976365358\n",
            "Cost after epoch 4200: 18.369939002565403\n",
            "Cost after epoch 4220: 18.37519404408085\n",
            "Cost after epoch 4240: 18.36018684468333\n",
            "Cost after epoch 4260: 18.388699693895205\n",
            "Cost after epoch 4280: 18.395993947927032\n",
            "Cost after epoch 4300: 18.371218031524222\n",
            "Cost after epoch 4320: 18.40815702951759\n",
            "Cost after epoch 4340: 18.41131959639368\n",
            "Cost after epoch 4360: 18.408415038790917\n",
            "Cost after epoch 4380: 18.423599703305644\n",
            "Cost after epoch 4400: 18.391315005246273\n",
            "Cost after epoch 4420: 18.418610270338295\n",
            "Cost after epoch 4440: 18.40543642668\n",
            "Cost after epoch 4460: 18.41018633912782\n",
            "Cost after epoch 4480: 18.43257064902679\n",
            "Cost after epoch 4500: 18.42756293381735\n",
            "Cost after epoch 4520: 18.432272837291833\n",
            "Cost after epoch 4540: 18.422783803975697\n",
            "Cost after epoch 4560: 18.40790082598296\n",
            "Cost after epoch 4580: 18.430704190183715\n",
            "Cost after epoch 4600: 18.426154272918872\n",
            "Cost after epoch 4620: 18.39789679438397\n",
            "Cost after epoch 4640: 18.397065952159853\n",
            "Cost after epoch 4660: 18.39578967703512\n",
            "Cost after epoch 4680: 18.399384151461476\n",
            "Cost after epoch 4700: 18.408206218971923\n",
            "Cost after epoch 4720: 18.368351377948795\n",
            "Cost after epoch 4740: 18.36390322389162\n",
            "Cost after epoch 4760: 18.3925705987328\n",
            "Cost after epoch 4780: 18.367164148961262\n",
            "Cost after epoch 4800: 18.38244960686998\n",
            "Cost after epoch 4820: 18.35077337163915\n",
            "Cost after epoch 4840: 18.36327979694235\n",
            "Cost after epoch 4860: 18.36141061195289\n",
            "Cost after epoch 4880: 18.36437595831985\n",
            "Cost after epoch 4900: 18.350563170479717\n",
            "Cost after epoch 4920: 18.34771544705716\n",
            "Cost after epoch 4940: 18.3606430162633\n",
            "Cost after epoch 4960: 18.349002190613337\n",
            "Cost after epoch 4980: 18.360511750913066\n",
            "Cost after epoch 5000: 18.333194526203684\n",
            "Cost after epoch 5020: 18.326883192212684\n",
            "Cost after epoch 5040: 18.322662794181383\n",
            "Cost after epoch 5060: 18.361218857835542\n",
            "Cost after epoch 5080: 18.333882919892613\n",
            "Cost after epoch 5100: 18.332992123431957\n",
            "Cost after epoch 5120: 18.327193454496758\n",
            "Cost after epoch 5140: 18.338823046058614\n",
            "Cost after epoch 5160: 18.349461035189808\n",
            "Cost after epoch 5180: 18.339587515107464\n",
            "Cost after epoch 5200: 18.32084806216714\n",
            "Cost after epoch 5220: 18.334051131351977\n",
            "Cost after epoch 5240: 18.329133350439186\n",
            "Cost after epoch 5260: 18.33119328542824\n",
            "Cost after epoch 5280: 18.314320096459383\n",
            "Cost after epoch 5300: 18.331453072079228\n",
            "Cost after epoch 5320: 18.33453721785231\n",
            "Cost after epoch 5340: 18.342644999726783\n",
            "Cost after epoch 5360: 18.33890538868969\n",
            "Cost after epoch 5380: 18.35054121953906\n",
            "Cost after epoch 5400: 18.341796031549947\n",
            "Cost after epoch 5420: 18.320879128849\n",
            "Cost after epoch 5440: 18.319735838866016\n",
            "Cost after epoch 5460: 18.328573671920328\n",
            "Cost after epoch 5480: 18.33838728239016\n",
            "Cost after epoch 5500: 18.35005149226343\n",
            "Cost after epoch 5520: 18.323581378856307\n",
            "Cost after epoch 5540: 18.32501579876861\n",
            "Cost after epoch 5560: 18.35523319454471\n",
            "Cost after epoch 5580: 18.328015532471312\n",
            "Cost after epoch 5600: 18.35068838869742\n",
            "Cost after epoch 5620: 18.326393572842395\n",
            "Cost after epoch 5640: 18.336844632852703\n",
            "Cost after epoch 5660: 18.354604004290135\n",
            "Cost after epoch 5680: 18.35422807011432\n",
            "Cost after epoch 5700: 18.360554679598664\n",
            "Cost after epoch 5720: 18.32452062191691\n",
            "Cost after epoch 5740: 18.32751003868653\n",
            "Cost after epoch 5760: 18.35020247777639\n",
            "Cost after epoch 5780: 18.353901276734423\n",
            "Cost after epoch 5800: 18.325144155937046\n",
            "Cost after epoch 5820: 18.34785397616117\n",
            "Cost after epoch 5840: 18.34258822922986\n",
            "Cost after epoch 5860: 18.36747438645922\n",
            "Cost after epoch 5880: 18.34962573413949\n",
            "Cost after epoch 5900: 18.367630421788622\n",
            "Cost after epoch 5920: 18.34722899660798\n",
            "Cost after epoch 5940: 18.358838342317377\n",
            "Cost after epoch 5960: 18.332423547429322\n",
            "Cost after epoch 5980: 18.365799920778628\n",
            "Cost after epoch 6000: 18.34400674304916\n",
            "Cost after epoch 6020: 18.359146744865736\n",
            "Cost after epoch 6040: 18.34007439798061\n",
            "Cost after epoch 6060: 18.33970452990429\n",
            "Cost after epoch 6080: 18.360480977044563\n",
            "Cost after epoch 6100: 18.331307123419517\n",
            "Cost after epoch 6120: 18.350104285757475\n",
            "Cost after epoch 6140: 18.353040704108597\n",
            "Cost after epoch 6160: 18.327756740770827\n",
            "Cost after epoch 6180: 18.318522816469148\n",
            "Cost after epoch 6200: 18.3450138178213\n",
            "Cost after epoch 6220: 18.33170358917797\n",
            "Cost after epoch 6240: 18.33011579618423\n",
            "Cost after epoch 6260: 18.31955414306611\n",
            "Cost after epoch 6280: 18.327702650402284\n",
            "Cost after epoch 6300: 18.292078810522085\n",
            "Cost after epoch 6320: 18.29615026436076\n",
            "Cost after epoch 6340: 18.291551336461676\n",
            "Cost after epoch 6360: 18.301342658890718\n",
            "Cost after epoch 6380: 18.303612894591623\n",
            "Cost after epoch 6400: 18.308013196880218\n",
            "Cost after epoch 6420: 18.287054769123063\n",
            "Cost after epoch 6440: 18.266898227075075\n",
            "Cost after epoch 6460: 18.26514036368769\n",
            "Cost after epoch 6480: 18.26636675299118\n",
            "Cost after epoch 6500: 18.282064088122706\n",
            "Cost after epoch 6520: 18.27291530795838\n",
            "Cost after epoch 6540: 18.253680053076444\n",
            "Cost after epoch 6560: 18.244296109673666\n",
            "Cost after epoch 6580: 18.264560117903688\n",
            "Cost after epoch 6600: 18.239902122525134\n",
            "Cost after epoch 6620: 18.239041614250823\n",
            "Cost after epoch 6640: 18.2643957719575\n",
            "Cost after epoch 6660: 18.2605261214716\n",
            "Cost after epoch 6680: 18.257039158443\n",
            "Cost after epoch 6700: 18.257411176775115\n",
            "Cost after epoch 6720: 18.24685165219603\n",
            "Cost after epoch 6740: 18.25110144501995\n",
            "Cost after epoch 6760: 18.24488436023361\n",
            "Cost after epoch 6780: 18.242632705216863\n",
            "Cost after epoch 6800: 18.253989323437725\n",
            "Cost after epoch 6820: 18.253114502086767\n",
            "Cost after epoch 6840: 18.24906881993\n",
            "Cost after epoch 6860: 18.235053129952583\n",
            "Cost after epoch 6880: 18.238386271080525\n",
            "Cost after epoch 6900: 18.235871899387966\n",
            "Cost after epoch 6920: 18.23000513810529\n",
            "Cost after epoch 6940: 18.25328659474828\n",
            "Cost after epoch 6960: 18.245683976716943\n",
            "Cost after epoch 6980: 18.2612369225058\n",
            "Cost after epoch 7000: 18.230152113011794\n",
            "Cost after epoch 7020: 18.238657980549988\n",
            "Cost after epoch 7040: 18.242147887748644\n",
            "Cost after epoch 7060: 18.244265843746227\n",
            "Cost after epoch 7080: 18.238490363868213\n",
            "Cost after epoch 7100: 18.258558940315382\n",
            "Cost after epoch 7120: 18.25703809466287\n",
            "Cost after epoch 7140: 18.24767246439366\n",
            "Cost after epoch 7160: 18.2529483798344\n",
            "Cost after epoch 7180: 18.249146006112014\n",
            "Cost after epoch 7200: 18.244965676606522\n",
            "Cost after epoch 7220: 18.240788143608757\n",
            "Cost after epoch 7240: 18.251783837112484\n",
            "Cost after epoch 7260: 18.266569680089596\n",
            "Cost after epoch 7280: 18.24280306751574\n",
            "Cost after epoch 7300: 18.243403647109616\n",
            "Cost after epoch 7320: 18.24473380097722\n",
            "Cost after epoch 7340: 18.24900499079869\n",
            "Cost after epoch 7360: 18.255673662048398\n",
            "Cost after epoch 7380: 18.243675245651968\n",
            "Cost after epoch 7400: 18.259220034392595\n",
            "Cost after epoch 7420: 18.23315430284438\n",
            "Cost after epoch 7440: 18.238389377119578\n",
            "Cost after epoch 7460: 18.244688391704827\n",
            "Cost after epoch 7480: 18.252368766371603\n",
            "Cost after epoch 7500: 18.24485408451543\n",
            "Cost after epoch 7520: 18.227962768188235\n",
            "Cost after epoch 7540: 18.24195364904873\n",
            "Cost after epoch 7560: 18.250741241297757\n",
            "Cost after epoch 7580: 18.238441968495824\n",
            "Cost after epoch 7600: 18.24618500718149\n",
            "Cost after epoch 7620: 18.23615870822492\n",
            "Cost after epoch 7640: 18.23537251194411\n",
            "Cost after epoch 7660: 18.237281419088127\n",
            "Cost after epoch 7680: 18.231650053026765\n",
            "Cost after epoch 7700: 18.230955660769954\n",
            "Cost after epoch 7720: 18.233980323043102\n",
            "Cost after epoch 7740: 18.23764168962268\n",
            "Cost after epoch 7760: 18.2421945786479\n",
            "Cost after epoch 7780: 18.252818398056114\n",
            "Cost after epoch 7800: 18.241631631599095\n",
            "Cost after epoch 7820: 18.251799328223335\n",
            "Cost after epoch 7840: 18.23355010018913\n",
            "Cost after epoch 7860: 18.25565626408215\n",
            "Cost after epoch 7880: 18.253902212731177\n",
            "Cost after epoch 7900: 18.24048577127637\n",
            "Cost after epoch 7920: 18.247265656446647\n",
            "Cost after epoch 7940: 18.239077491631424\n",
            "Cost after epoch 7960: 18.25249361883673\n",
            "Cost after epoch 7980: 18.249923594699872\n",
            "Cost after epoch 8000: 18.257550599146835\n",
            "Cost after epoch 8020: 18.23846748973179\n",
            "Cost after epoch 8040: 18.247481749089907\n",
            "Cost after epoch 8060: 18.24464957004013\n",
            "Cost after epoch 8080: 18.2588511302518\n",
            "Cost after epoch 8100: 18.253449882015808\n",
            "Cost after epoch 8120: 18.25166742225901\n",
            "Cost after epoch 8140: 18.247063794455258\n",
            "Cost after epoch 8160: 18.247452882359116\n",
            "Cost after epoch 8180: 18.258431103708283\n",
            "Cost after epoch 8200: 18.244604381890248\n",
            "Cost after epoch 8220: 18.2527131960333\n",
            "Cost after epoch 8240: 18.246118686396965\n",
            "Cost after epoch 8260: 18.24767600167837\n",
            "Cost after epoch 8280: 18.260177199157507\n",
            "Cost after epoch 8300: 18.26193060072951\n",
            "Cost after epoch 8320: 18.24694366896299\n",
            "Cost after epoch 8340: 18.253500031425464\n",
            "Cost after epoch 8360: 18.24654684955751\n",
            "Cost after epoch 8380: 18.26123402314852\n",
            "Cost after epoch 8400: 18.2669655199899\n",
            "Cost after epoch 8420: 18.261965054057328\n",
            "Cost after epoch 8440: 18.25213539725278\n",
            "Cost after epoch 8460: 18.264407552268406\n",
            "Cost after epoch 8480: 18.253410419557873\n",
            "Cost after epoch 8500: 18.262637006022366\n",
            "Cost after epoch 8520: 18.262481937508845\n",
            "Cost after epoch 8540: 18.24434645233723\n",
            "Cost after epoch 8560: 18.25803225584582\n",
            "Cost after epoch 8580: 18.251510599755814\n",
            "Cost after epoch 8600: 18.261182686468672\n",
            "Cost after epoch 8620: 18.253135047833332\n",
            "Cost after epoch 8640: 18.244320999947213\n",
            "Cost after epoch 8660: 18.260291477867003\n",
            "Cost after epoch 8680: 18.258781602389274\n",
            "Cost after epoch 8700: 18.24522572623917\n",
            "Cost after epoch 8720: 18.24931017503202\n",
            "Cost after epoch 8740: 18.244079345223305\n",
            "Cost after epoch 8760: 18.240126462011528\n",
            "Cost after epoch 8780: 18.248782947196347\n",
            "Cost after epoch 8800: 18.25127670666417\n",
            "Cost after epoch 8820: 18.24756926621564\n",
            "Cost after epoch 8840: 18.247935443984897\n",
            "Cost after epoch 8860: 18.24266052280498\n",
            "Cost after epoch 8880: 18.24569113176685\n",
            "Cost after epoch 8900: 18.251598547575703\n",
            "Cost after epoch 8920: 18.22492079716356\n",
            "Cost after epoch 8940: 18.231613170431835\n",
            "Cost after epoch 8960: 18.240189737202037\n",
            "Cost after epoch 8980: 18.231441896717307\n",
            "Cost after epoch 9000: 18.23592752148772\n",
            "Cost after epoch 9020: 18.238507210471397\n",
            "Cost after epoch 9040: 18.23390927232428\n",
            "Cost after epoch 9060: 18.227677604376932\n",
            "Cost after epoch 9080: 18.22869870572061\n",
            "Cost after epoch 9100: 18.23183396211027\n",
            "Cost after epoch 9120: 18.224754516290968\n",
            "Cost after epoch 9140: 18.21670264689666\n",
            "Cost after epoch 9160: 18.2082973822473\n",
            "Cost after epoch 9180: 18.228875117566183\n",
            "Cost after epoch 9200: 18.21191748266947\n",
            "Cost after epoch 9220: 18.221687972302256\n",
            "Cost after epoch 9240: 18.216569040315548\n",
            "Cost after epoch 9260: 18.212800226644667\n",
            "Cost after epoch 9280: 18.2095813850622\n",
            "Cost after epoch 9300: 18.220878339662953\n",
            "Cost after epoch 9320: 18.210432157375436\n",
            "Cost after epoch 9340: 18.207537465162062\n",
            "Cost after epoch 9360: 18.204957681543064\n",
            "Cost after epoch 9380: 18.21160942784102\n",
            "Cost after epoch 9400: 18.205521752838468\n",
            "Cost after epoch 9420: 18.201553417484135\n",
            "Cost after epoch 9440: 18.209444017771766\n",
            "Cost after epoch 9460: 18.20073680038576\n",
            "Cost after epoch 9480: 18.21406984649209\n",
            "Cost after epoch 9500: 18.203410654937237\n",
            "Cost after epoch 9520: 18.209036363953555\n",
            "Cost after epoch 9540: 18.198364669914817\n",
            "Cost after epoch 9560: 18.208300808721106\n",
            "Cost after epoch 9580: 18.200537023065728\n",
            "Cost after epoch 9600: 18.200142583351546\n",
            "Cost after epoch 9620: 18.20626690873402\n",
            "Cost after epoch 9640: 18.210763046662468\n",
            "Cost after epoch 9660: 18.2111854456156\n",
            "Cost after epoch 9680: 18.214996908707416\n",
            "Cost after epoch 9700: 18.201796464353798\n",
            "Cost after epoch 9720: 18.211996818688878\n",
            "Cost after epoch 9740: 18.20068599356106\n",
            "Cost after epoch 9760: 18.207231791816263\n",
            "Cost after epoch 9780: 18.20368493218065\n",
            "Cost after epoch 9800: 18.206900678224663\n",
            "Cost after epoch 9820: 18.21655538413068\n",
            "Cost after epoch 9840: 18.20860707791742\n",
            "Cost after epoch 9860: 18.20834726848739\n",
            "Cost after epoch 9880: 18.209569993004315\n",
            "Cost after epoch 9900: 18.218300442947502\n",
            "Cost after epoch 9920: 18.209403704090825\n",
            "Cost after epoch 9940: 18.223160938618573\n",
            "Cost after epoch 9960: 18.22538059562058\n",
            "Cost after epoch 9980: 18.21530735484444\n",
            "training time: 0:01:26.420842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5zowXVw8VKT"
      },
      "source": [
        "X_test = np.arange(vocab_size)\n",
        "X_test = np.expand_dims(X_test, axis=0)\n",
        "softmax_test, _ = forward_propagation(X_test, paras)\n",
        "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:, :]\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSn34oS-nM0",
        "outputId": "57bcf96a-3cb3-4595-98f8-828b3ba956cf"
      },
      "source": [
        "for input_ind in range(vocab_size):\n",
        "    input_word = id_to_word[input_ind]\n",
        "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
        "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this's neighbor words: ['input', 'us', 'case', 'helps']\n",
            "just's neighbor words: ['but', 'embedding', 'lines', 'one']\n",
            "two's neighbor words: ['put', 'words', 'three', 'one']\n",
            "called's neighbor words: ['of', 'word', 'we', 'model']\n",
            "which's neighbor words: ['required', 'nuerons', 'are', 'hidden']\n",
            "input's neighbor words: ['good', 'with', 'given', 'not']\n",
            "continuous's neighbor words: ['words', 'it', 'bag', 'called']\n",
            "lines's neighbor words: ['a', 'which', 'the', 'of']\n",
            "in's neighbor words: ['model', 'the', 'skip', 'this']\n",
            "embeddings's neighbor words: ['particular', 'are', 'required', 'of']\n",
            "other's neighbor words: ['word2vec', 'variant', 'vector', 'one']\n",
            "of's neighbor words: ['the', 'word', 'a', 'input']\n",
            "cbow's neighbor words: ['helps', 'or', 'words', 'variant']\n",
            "good's neighbor words: ['accuracy', 'input', 'with', 'sequence']\n",
            "nuerons's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "is's neighbor words: ['one', 'predict', 'bag', 'it']\n",
            "it's neighbor words: ['continuous', 'inputs', 'called', 'is']\n",
            "next's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "converges's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "a's neighbor words: ['the', 'approach', 'since', 'different']\n",
            "works's neighbor words: ['same', 'word2vec', 'these', 'on']\n",
            "once's neighbor words: ['converges', 'skip', 'gram', 'model']\n",
            "get's neighbor words: ['of', 'word', 'we', 'model']\n",
            "on's neighbor words: ['lines', 'works', 'these', 'word2vec']\n",
            "predict's neighbor words: ['vector', 'word', 'is', 'us']\n",
            "not's neighbor words: ['hot', 'one', 'is', 'layer']\n",
            "or's neighbor words: ['variant', 'bag', 'cbow', 'this']\n",
            "these's neighbor words: ['but', 'embedding', 'lines', 'one']\n",
            "trained's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "consider's neighbor words: ['of', 'word', 'we', 'model']\n",
            "same's neighbor words: ['with', 'sequence', 'skip', 'as']\n",
            "embedding's neighbor words: ['just', 'hot', 'one', 'is']\n",
            "vector's neighbor words: ['one', 'in', 'as', 'probability']\n",
            "words's neighbor words: ['this', 'i', 'put', 'together']\n",
            "i's neighbor words: ['sequence', 'together', 'put', 'words']\n",
            "model's neighbor words: ['once', 'these', 'on', 'model']\n",
            "one's neighbor words: ['two', 'vector', 'word', 'just']\n",
            "sequence's neighbor words: ['accuracy', 'good', 'given', 'with']\n",
            "hidden's neighbor words: ['which', 'nuerons', 'weights', 'layer']\n",
            "us's neighbor words: ['next', 'this', 'helps', 'predict']\n",
            "with's neighbor words: ['accuracy', 'good', 'given', 'sequence']\n",
            "skip's neighbor words: ['once', 'gram', 'as', 'in']\n",
            "we's neighbor words: ['sequence', 'weights', 'since', 'get']\n",
            "helps's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "given's neighbor words: ['with', 'sequence', 'word', 'input']\n",
            "put's neighbor words: ['e', 'three', 'two', 'i']\n",
            "case's neighbor words: ['layer', 'approach', 'this', 'nuerons']\n",
            "bag's neighbor words: ['or', 'continuous', 'called', 'cbow']\n",
            "word's neighbor words: ['of', 'given', 'three', 'consider']\n",
            "probability's neighbor words: ['as', 'to', 'vector', 'predict']\n",
            "gram's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "approach's neighbor words: ['case', 'different', 'slightly', 'this']\n",
            "e's neighbor words: ['then', 'put', 'i', 'together']\n",
            "inputs's neighbor words: ['called', 'it', 'a', 'e']\n",
            "are's neighbor words: ['embeddings', 'required', 'layer', 'which']\n",
            "but's neighbor words: ['words', 'slightly', 'three', 'two']\n",
            "slightly's neighbor words: ['in', 'approach', 'different', 'but']\n",
            "word2vec's neighbor words: ['on', 'works', 'other', 'variant']\n",
            "layer's neighbor words: ['the', 'are', 'a', 'which']\n",
            "weights's neighbor words: ['hidden', 'get', 'we', 'nuerons']\n",
            "required's neighbor words: ['a', 'the', 'which', 'of']\n",
            "particular's neighbor words: ['we', 'since', 'embeddings', 'vector']\n",
            "accuracy's neighbor words: ['good', 'sequence', 'with', 'of']\n",
            "together's neighbor words: ['a', 'the', 'which', 'of']\n",
            "as's neighbor words: ['gram', 'skip', 'probability', 'in']\n",
            "different's neighbor words: ['this', 'slightly', 'approach', 'in']\n",
            "since's neighbor words: ['a', 'the', 'which', 'of']\n",
            "to's neighbor words: ['probability', 'trained', 'predict', 'sequence']\n",
            "the's neighbor words: ['sequence', 'of', 'input', 'is']\n",
            "hot's neighbor words: ['of', 'word', 'we', 'model']\n",
            "then's neighbor words: ['is', 'e', 'this', 'model']\n",
            "variant's neighbor words: ['model', 'predict', 'word2vec', 'us']\n",
            "three's neighbor words: ['together', 'two', 'put', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA-Bp9pQ_IXg"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    }
  ]
}