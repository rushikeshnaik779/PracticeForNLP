{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_skipgram v1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_snw9LdxQ6Q5"
      },
      "source": [
        "%load_ext autoreload \n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L9QUJXERzmI"
      },
      "source": [
        "import numpy as np\n",
        "import re \n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u83cdz_kSskr"
      },
      "source": [
        "# Data Preparation \n",
        "ReInventing the wheel is usually an awesome way to learn something deeply"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8D_ppgPSTxc"
      },
      "source": [
        "def tokenize(text):\n",
        "    # obtains tokens with atlest 1 alphabet \n",
        "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
        "    return pattern.findall(text.lower())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81u2I1dFUL0S",
        "outputId": "510224d1-12a9-4190-fa1f-0c3d05563dde"
      },
      "source": [
        "tokenize('I love goa')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'goa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3eFAOaNUO9B"
      },
      "source": [
        "def mapping(tokens):\n",
        "    word_to_id = dict()\n",
        "    id_to_word = dict()\n",
        "    \n",
        "    for i, token in enumerate(set(tokens)):\n",
        "        word_to_id[token] = i\n",
        "        id_to_word[i] = token\n",
        "\n",
        "    \n",
        "    return word_to_id, id_to_word "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_plZiwKVlPO",
        "outputId": "e396b996-6bee-4ef7-b52f-d4907d86eb4a"
      },
      "source": [
        "mapping(['i', 'love', 'goa'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'goa': 1, 'i': 0, 'love': 2}, {0: 'i', 1: 'goa', 2: 'love'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_0dS7CAVojc"
      },
      "source": [
        "def generating_training_data(tokens, word_to_id, window_size):\n",
        "    N = len(tokens)\n",
        "    X, Y = [], []\n",
        "    for i in range(N):\n",
        "        nbr_inds = list(range(max(0, i-window_size), i)) + \\\n",
        "                    list(range(i+1, min(N, i+window_size + 1)))\n",
        "        for j in nbr_inds:\n",
        "            X.append(word_to_id[tokens[i]])\n",
        "            Y.append(word_to_id[tokens[j]])\n",
        "        \n",
        "    X = np.array(X)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    Y = np.array(Y)\n",
        "    Y = np.expand_dims(Y, axis=0)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXeZScexWu5P",
        "outputId": "f7839b64-b04e-4912-f34f-f5c7a2caf0ea"
      },
      "source": [
        "generating_training_data(['i', 'love', 'goa'], {'goa': 2, 'i': 1, 'love': 0}, 2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1, 1, 0, 0, 2, 2]]), array([[0, 2, 1, 2, 1, 0]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Jc-lzSYF2H"
      },
      "source": [
        "doc = \"\"\"\n",
        "The other variant of Word2Vec model works on these same lines but with a slightly different approach. In this case, the input layer is not a one hot embedding vector of just one word but two-three words put together i.e. a sequence. Then the model is trained to predict the probability vector, same as in skip-gram model. Once the model converges, we get the weights of the hidden layer nuerons which are the required embeddings of a particular word. Since, we consider a sequence of inputs, it is called ‘continuous bag of words or CBOW’. This variant helps us predict the next word of the given input sequence with good accuracy.\n",
        "\"\"\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfq5MvL1YiKG",
        "outputId": "936ab77d-cdf9-49db-c1cc-5b0d1452beb9"
      },
      "source": [
        "tokens = tokenize(doc)\n",
        "print(tokens)\n",
        "word_to_id, id_to_word = mapping(tokens)\n",
        "X, Y = generating_training_data(tokens, word_to_id, 3)\n",
        "vocab_size = len(id_to_word)\n",
        "m = Y.shape[1]\n",
        "# turn Y into one hot encoding \n",
        "Y_one_hot = np.zeros((vocab_size, m))\n",
        "Y_one_hot[Y.flatten(), np.arange(m)]=1"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'other', 'variant', 'of', 'word2vec', 'model', 'works', 'on', 'these', 'same', 'lines', 'but', 'with', 'a', 'slightly', 'different', 'approach', 'in', 'this', 'case', 'the', 'input', 'layer', 'is', 'not', 'a', 'one', 'hot', 'embedding', 'vector', 'of', 'just', 'one', 'word', 'but', 'two', 'three', 'words', 'put', 'together', 'i', 'e', 'a', 'sequence', 'then', 'the', 'model', 'is', 'trained', 'to', 'predict', 'the', 'probability', 'vector', 'same', 'as', 'in', 'skip', 'gram', 'model', 'once', 'the', 'model', 'converges', 'we', 'get', 'the', 'weights', 'of', 'the', 'hidden', 'layer', 'nuerons', 'which', 'are', 'the', 'required', 'embeddings', 'of', 'a', 'particular', 'word', 'since', 'we', 'consider', 'a', 'sequence', 'of', 'inputs', 'it', 'is', 'called', 'continuous', 'bag', 'of', 'words', 'or', 'cbow', 'this', 'variant', 'helps', 'us', 'predict', 'the', 'next', 'word', 'of', 'the', 'given', 'input', 'sequence', 'with', 'good', 'accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nCNkt4LZMiV",
        "outputId": "d59f11d2-0286-40c6-90a2-a671ad3a5658"
      },
      "source": [
        "print(X, Y)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[68 68 68 10 10 10 10 71 71 71 71 71 11 11 11 11 11 11 57 57 57 57 57 57\n",
            "  35 35 35 35 35 35 20 20 20 20 20 20 23 23 23 23 23 23 27 27 27 27 27 27\n",
            "  30 30 30 30 30 30  7  7  7  7  7  7 55 55 55 55 55 55 40 40 40 40 40 40\n",
            "  19 19 19 19 19 19 56 56 56 56 56 56 65 65 65 65 65 65 51 51 51 51 51 51\n",
            "   8  8  8  8  8  8  0  0  0  0  0  0 46 46 46 46 46 46 68 68 68 68 68 68\n",
            "   5  5  5  5  5  5 58 58 58 58 58 58 15 15 15 15 15 15 25 25 25 25 25 25\n",
            "  19 19 19 19 19 19 36 36 36 36 36 36 69 69 69 69 69 69 31 31 31 31 31 31\n",
            "  32 32 32 32 32 32 11 11 11 11 11 11  1  1  1  1  1  1 36 36 36 36 36 36\n",
            "  48 48 48 48 48 48 55 55 55 55 55 55  2  2  2  2  2  2 72 72 72 72 72 72\n",
            "  33 33 33 33 33 33 45 45 45 45 45 45 63 63 63 63 63 63 34 34 34 34 34 34\n",
            "  52 52 52 52 52 52 19 19 19 19 19 19 37 37 37 37 37 37 70 70 70 70 70 70\n",
            "  68 68 68 68 68 68 35 35 35 35 35 35 15 15 15 15 15 15 28 28 28 28 28 28\n",
            "  67 67 67 67 67 67 24 24 24 24 24 24 68 68 68 68 68 68 49 49 49 49 49 49\n",
            "  32 32 32 32 32 32 30 30 30 30 30 30 64 64 64 64 64 64  8  8  8  8  8  8\n",
            "  41 41 41 41 41 41 50 50 50 50 50 50 35 35 35 35 35 35 21 21 21 21 21 21\n",
            "  68 68 68 68 68 68 35 35 35 35 35 35 18 18 18 18 18 18 42 42 42 42 42 42\n",
            "  22 22 22 22 22 22 68 68 68 68 68 68 59 59 59 59 59 59 11 11 11 11 11 11\n",
            "  68 68 68 68 68 68 38 38 38 38 38 38 58 58 58 58 58 58 14 14 14 14 14 14\n",
            "   4  4  4  4  4  4 54 54 54 54 54 54 68 68 68 68 68 68 60 60 60 60 60 60\n",
            "   9  9  9  9  9  9 11 11 11 11 11 11 19 19 19 19 19 19 61 61 61 61 61 61\n",
            "  48 48 48 48 48 48 66 66 66 66 66 66 42 42 42 42 42 42 29 29 29 29 29 29\n",
            "  19 19 19 19 19 19 37 37 37 37 37 37 11 11 11 11 11 11 53 53 53 53 53 53\n",
            "  16 16 16 16 16 16 15 15 15 15 15 15  3  3  3  3  3  3  6  6  6  6  6  6\n",
            "  47 47 47 47 47 47 11 11 11 11 11 11 33 33 33 33 33 33 26 26 26 26 26 26\n",
            "  12 12 12 12 12 12  0  0  0  0  0  0 71 71 71 71 71 71 43 43 43 43 43 43\n",
            "  39 39 39 39 39 39 24 24 24 24 24 24 68 68 68 68 68 68 17 17 17 17 17 17\n",
            "  48 48 48 48 48 48 11 11 11 11 11 11 68 68 68 68 68 68 44 44 44 44 44 44\n",
            "   5  5  5  5  5  5 37 37 37 37 37 37 40 40 40 40 40 13 13 13 13 62 62 62]] [[10 71 11 68 71 11 57 68 10 11 57 35 68 10 71 57 35 20 10 71 11 35 20 23\n",
            "  71 11 57 20 23 27 11 57 35 23 27 30 57 35 20 27 30  7 35 20 23 30  7 55\n",
            "  20 23 27  7 55 40 23 27 30 55 40 19 27 30  7 40 19 56 30  7 55 19 56 65\n",
            "   7 55 40 56 65 51 55 40 19 65 51  8 40 19 56 51  8  0 19 56 65  8  0 46\n",
            "  56 65 51  0 46 68 65 51  8 46 68  5 51  8  0 68  5 58  8  0 46  5 58 15\n",
            "   0 46 68 58 15 25 46 68  5 15 25 19 68  5 58 25 19 36  5 58 15 19 36 69\n",
            "  58 15 25 36 69 31 15 25 19 69 31 32 25 19 36 31 32 11 19 36 69 32 11  1\n",
            "  36 69 31 11  1 36 69 31 32  1 36 48 31 32 11 36 48 55 32 11  1 48 55  2\n",
            "  11  1 36 55  2 72  1 36 48  2 72 33 36 48 55 72 33 45 48 55  2 33 45 63\n",
            "  55  2 72 45 63 34  2 72 33 63 34 52 72 33 45 34 52 19 33 45 63 52 19 37\n",
            "  45 63 34 19 37 70 63 34 52 37 70 68 34 52 19 70 68 35 52 19 37 68 35 15\n",
            "  19 37 70 35 15 28 37 70 68 15 28 67 70 68 35 28 67 24 68 35 15 67 24 68\n",
            "  35 15 28 24 68 49 15 28 67 68 49 32 28 67 24 49 32 30 67 24 68 32 30 64\n",
            "  24 68 49 30 64  8 68 49 32 64  8 41 49 32 30  8 41 50 32 30 64 41 50 35\n",
            "  30 64  8 50 35 21 64  8 41 35 21 68  8 41 50 21 68 35 41 50 35 68 35 18\n",
            "  50 35 21 35 18 42 35 21 68 18 42 22 21 68 35 42 22 68 68 35 18 22 68 59\n",
            "  35 18 42 68 59 11 18 42 22 59 11 68 42 22 68 11 68 38 22 68 59 68 38 58\n",
            "  68 59 11 38 58 14 59 11 68 58 14  4 11 68 38 14  4 54 68 38 58  4 54 68\n",
            "  38 58 14 54 68 60 58 14  4 68 60  9 14  4 54 60  9 11  4 54 68  9 11 19\n",
            "  54 68 60 11 19 61 68 60  9 19 61 48 60  9 11 61 48 66  9 11 19 48 66 42\n",
            "  11 19 61 66 42 29 19 61 48 42 29 19 61 48 66 29 19 37 48 66 42 19 37 11\n",
            "  66 42 29 37 11 53 42 29 19 11 53 16 29 19 37 53 16 15 19 37 11 16 15  3\n",
            "  37 11 53 15  3  6 11 53 16  3  6 47 53 16 15  6 47 11 16 15  3 47 11 33\n",
            "  15  3  6 11 33 26  3  6 47 33 26 12  6 47 11 26 12  0 47 11 33 12  0 71\n",
            "  11 33 26  0 71 43 33 26 12 71 43 39 26 12  0 43 39 24 12  0 71 39 24 68\n",
            "   0 71 43 24 68 17 71 43 39 68 17 48 43 39 24 17 48 11 39 24 68 48 11 68\n",
            "  24 68 17 11 68 44 68 17 48 68 44  5 17 48 11 44  5 37 48 11 68  5 37 40\n",
            "  11 68 44 37 40 13 68 44  5 40 13 62 44  5 37 13 62  5 37 40 62 37 40 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9l_uzkZORL",
        "outputId": "2d77600a-425c-48cf-c24a-d4bef05103f2"
      },
      "source": [
        "print(Y_one_hot)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzkskEVcaVav"
      },
      "source": [
        "#Initialization \n",
        "* word Embedding \n",
        "* Dense Layer \n",
        "* Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM4csjGbaE-8"
      },
      "source": [
        "def initialize_wrd_emb(vocab_size, emb_size):\n",
        "    WRD_EMB = np.random.randn(vocab_size, emb_size)*0.01\n",
        "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
        "    return WRD_EMB"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npIjTK4MbBcl"
      },
      "source": [
        "def initialize_dense(input_size, output_size):\n",
        "    W = np.random.randn(output_size, input_size) * 0.01\n",
        "\n",
        "    assert(W.shape == (output_size, input_size))\n",
        "\n",
        "    return W"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9l5EsMbj9F"
      },
      "source": [
        "def initialize_parameters(vocab_size, emb_size):\n",
        "    WRD_EMB  = initialize_wrd_emb(vocab_size, emb_size)\n",
        "    W = initialize_dense(emb_size, vocab_size)\n",
        "\n",
        "    parameters = {}\n",
        "    parameters['WRD_EMB'] = WRD_EMB \n",
        "    parameters['W'] = W\n",
        "    return parameters"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbqD0ZwucMO-",
        "outputId": "d14b0eee-4c17-4585-cd8d-cae89e74f74d"
      },
      "source": [
        "print(initialize_parameters(5, 5))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'WRD_EMB': array([[ 0.00797424, -0.00102398, -0.01053327,  0.01144288,  0.01087524],\n",
            "       [-0.0014608 ,  0.00014197, -0.00152098, -0.00103424, -0.01377535],\n",
            "       [ 0.01346266, -0.00090859,  0.00369364,  0.01376817, -0.0156851 ],\n",
            "       [ 0.01112075,  0.01053788, -0.00350727, -0.00291178, -0.01632778],\n",
            "       [ 0.00185193,  0.00080155,  0.00548211,  0.01214297,  0.00353309]]), 'W': array([[ 0.0163293 , -0.00488816, -0.00443603, -0.01363806, -0.01073577],\n",
            "       [-0.01489897, -0.01363439, -0.01334275,  0.01253328, -0.00738896],\n",
            "       [ 0.00059801, -0.00407217, -0.01031124,  0.00515161,  0.00789267],\n",
            "       [ 0.00186501, -0.01324542,  0.0141502 ,  0.00133391, -0.01127588],\n",
            "       [-0.00423727, -0.01283284,  0.00081796, -0.01422961,  0.00207391]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48v-0yxB15Co"
      },
      "source": [
        "# FORWARD PROPAGATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23wNFnCecSby"
      },
      "source": [
        "def int_to_word_vecs(inds, parameters):\n",
        "    \"\"\"\n",
        "    inds : numpy array. shape:(1, m)\n",
        "    parameters : dict. weights to be trained\n",
        "    \"\"\"\n",
        "    m = inds.shape[1]\n",
        "    WRD_EMB = parameters['WRD_EMB']\n",
        "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
        "\n",
        "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
        "\n",
        "    return word_vec\n",
        "\n",
        "\n",
        "\n",
        "def linear_dense(word_vec, parameters):\n",
        "    \"\"\"\n",
        "    word_vec : numpy array. shape : (emb_size, m)\n",
        "    parameters : dict. weights to be trained\n",
        "    \"\"\"\n",
        "\n",
        "    m = word_vec.shape[1]\n",
        "    W = parameters['W']\n",
        "    Z = np.dot(W, word_vec)\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], m))\n",
        "\n",
        "    return W, Z"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDhzHBM3oHi"
      },
      "source": [
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Z : output out of the dense layer. shape : (vocab_size, m)\n",
        "    \"\"\"\n",
        "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
        "\n",
        "    assert(softmax_out.shape == Z.shape)\n",
        "\n",
        "    return softmax_out"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQvm96GZ4VWa"
      },
      "source": [
        "def forward_propagation(inds, parameters):\n",
        "    word_vec = int_to_word_vecs(inds, parameters)\n",
        "    W, Z = linear_dense(word_vec, parameters)\n",
        "    softmax_out = softmax(Z)\n",
        "\n",
        "    caches = {}\n",
        "    caches['inds'] = inds\n",
        "    caches['word_vec'] = word_vec\n",
        "    caches['W'] = W\n",
        "    caches['Z'] = Z\n",
        "\n",
        "    return softmax_out, caches"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s20n7zdA4_zD"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_-LURsa5Cfe"
      },
      "source": [
        "# COST FUNCTION (CROSS ENTROPY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwbF9s2I5GNL"
      },
      "source": [
        "def cross_entropy(softmax_out, Y):\n",
        "    \"\"\"\n",
        "    softmax_out output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "\n",
        "    m = softmax_out.shape[1]\n",
        "\n",
        "    cost = -(1/m)*np.sum(np.sum(Y* np.log(softmax_out + 0.01), axis=0, keepdims=True), axis=1)\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3qbPxUs592f"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v93Wxihw6BNB"
      },
      "source": [
        "# BACKWARD PROPAGATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1FeqWt56DKW"
      },
      "source": [
        "def softmax_backward(Y, softmax_out):\n",
        "    \"\"\"\n",
        "    Y: labels of training data. shape:(vocab_size, m)\n",
        "    softmax_out : output out of softmax. shape: (vocab_size, m)\n",
        "    \"\"\"\n",
        "\n",
        "    dL_dZ = softmax_out - Y\n",
        "    assert(dL_dZ.shape == softmax_out.shape)\n",
        "\n",
        "    return dL_dZ"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McVdcLc16rZU"
      },
      "source": [
        "def dense_backward(dL_dZ, caches):\n",
        "    \"\"\"\n",
        "    dL_dZ : shape : (vocab_size, m)\n",
        "    caches: dict. results from each steps of forward propagation\n",
        "    \"\"\"\n",
        "    W = caches['W']\n",
        "    word_vec = caches['word_vec']\n",
        "    m = word_vec.shape[1]\n",
        "\n",
        "    dL_dW = (1/m) * np.dot(dL_dZ, word_vec.T)\n",
        "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
        "\n",
        "    assert( W.shape == dL_dW.shape)\n",
        "    assert( word_vec.shape == dL_dword_vec.shape)\n",
        "\n",
        "    return dL_dW, dL_dword_vec"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgxTY0wC8BD9"
      },
      "source": [
        "def backward_propagation(Y, softmax_out, caches):\n",
        "    dL_dZ = softmax_backward(Y, softmax_out)\n",
        "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
        "    \n",
        "    gradients = dict()\n",
        "    gradients['dL_dZ'] = dL_dZ\n",
        "    gradients['dL_dW'] = dL_dW\n",
        "    gradients['dL_dword_vec'] = dL_dword_vec\n",
        "    \n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, caches, gradients, learning_rate):\n",
        "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
        "    inds = caches['inds']\n",
        "    dL_dword_vec = gradients['dL_dword_vec']\n",
        "    m = inds.shape[-1]\n",
        "    \n",
        "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
        "\n",
        "    parameters['W'] -= learning_rate * gradients['dL_dW']"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS7t286q8PkB"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    if parameters is None:\n",
        "        parameters = initialize_parameters(vocab_size, emb_size)\n",
        "    \n",
        "    begin_time = datetime.now()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_cost = 0\n",
        "        batch_inds = list(range(0, m, batch_size))\n",
        "        np.random.shuffle(batch_inds)\n",
        "        for i in batch_inds:\n",
        "            X_batch = X[:, i:i+batch_size]\n",
        "            Y_batch = Y[:, i:i+batch_size]\n",
        "\n",
        "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
        "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
        "            update_parameters(parameters, caches, gradients, learning_rate)\n",
        "            cost = cross_entropy(softmax_out, Y_batch)\n",
        "            epoch_cost += np.squeeze(cost)\n",
        "            \n",
        "        costs.append(epoch_cost)\n",
        "        if print_cost and epoch % (epochs // 500) == 0:\n",
        "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
        "        if epoch % (epochs // 100) == 0:\n",
        "            learning_rate *= 0.98\n",
        "    end_time = datetime.now()\n",
        "    print('training time: {}'.format(end_time - begin_time))\n",
        "            \n",
        "    if plot_cost:\n",
        "        plt.plot(np.arange(epochs), costs)\n",
        "        plt.xlabel('# of epochs')\n",
        "        plt.ylabel('cost')\n",
        "    return parameters"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7y-9lnbt8TBh",
        "outputId": "c4fdfd05-7451-40e2-98ac-a2078f597d6d"
      },
      "source": [
        "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10000, batch_size=128, parameters=None, print_cost=True)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0: 22.45411373195214\n",
            "Cost after epoch 20: 22.45114314660615\n",
            "Cost after epoch 40: 22.446435662538942\n",
            "Cost after epoch 60: 22.437474660365204\n",
            "Cost after epoch 80: 22.419656673093527\n",
            "Cost after epoch 100: 22.38403126194479\n",
            "Cost after epoch 120: 22.3143377067294\n",
            "Cost after epoch 140: 22.177954850555192\n",
            "Cost after epoch 160: 21.92470198762704\n",
            "Cost after epoch 180: 21.536026536578532\n",
            "Cost after epoch 200: 21.13323590955621\n",
            "Cost after epoch 220: 20.795220874904256\n",
            "Cost after epoch 240: 20.52104779858792\n",
            "Cost after epoch 260: 20.288591190104253\n",
            "Cost after epoch 280: 20.078873538230276\n",
            "Cost after epoch 300: 19.87472224929181\n",
            "Cost after epoch 320: 19.654525655785317\n",
            "Cost after epoch 340: 19.43023229903674\n",
            "Cost after epoch 360: 19.201497985164597\n",
            "Cost after epoch 380: 18.954266699546817\n",
            "Cost after epoch 400: 18.700943459615697\n",
            "Cost after epoch 420: 18.463136727247843\n",
            "Cost after epoch 440: 18.233682555900238\n",
            "Cost after epoch 460: 18.031837993559027\n",
            "Cost after epoch 480: 17.858702233910535\n",
            "Cost after epoch 500: 17.694809924291825\n",
            "Cost after epoch 520: 17.565348062400925\n",
            "Cost after epoch 540: 17.46329830667533\n",
            "Cost after epoch 560: 17.364780324803718\n",
            "Cost after epoch 580: 17.27708226915464\n",
            "Cost after epoch 600: 17.22030538747844\n",
            "Cost after epoch 620: 17.163869887414762\n",
            "Cost after epoch 640: 17.111515171062873\n",
            "Cost after epoch 660: 17.06385810911157\n",
            "Cost after epoch 680: 17.029334078806077\n",
            "Cost after epoch 700: 16.980110923754765\n",
            "Cost after epoch 720: 16.93306989051148\n",
            "Cost after epoch 740: 16.904294775006264\n",
            "Cost after epoch 760: 16.87119101253967\n",
            "Cost after epoch 780: 16.837686837857312\n",
            "Cost after epoch 800: 16.826209318731145\n",
            "Cost after epoch 820: 16.79852033578224\n",
            "Cost after epoch 840: 16.757032265028883\n",
            "Cost after epoch 860: 16.751297764715858\n",
            "Cost after epoch 880: 16.723934827478207\n",
            "Cost after epoch 900: 16.730124803563577\n",
            "Cost after epoch 920: 16.703278837670446\n",
            "Cost after epoch 940: 16.705143272653338\n",
            "Cost after epoch 960: 16.701391873189824\n",
            "Cost after epoch 980: 16.697662313201924\n",
            "Cost after epoch 1000: 16.681128934380414\n",
            "Cost after epoch 1020: 16.691963554095995\n",
            "Cost after epoch 1040: 16.67621252755686\n",
            "Cost after epoch 1060: 16.679012183062916\n",
            "Cost after epoch 1080: 16.680317825133\n",
            "Cost after epoch 1100: 16.689001455526864\n",
            "Cost after epoch 1120: 16.68756073731265\n",
            "Cost after epoch 1140: 16.684840121252236\n",
            "Cost after epoch 1160: 16.6793925907184\n",
            "Cost after epoch 1180: 16.68472139441861\n",
            "Cost after epoch 1200: 16.681892116134826\n",
            "Cost after epoch 1220: 16.678819627886067\n",
            "Cost after epoch 1240: 16.710321505656637\n",
            "Cost after epoch 1260: 16.69163725499278\n",
            "Cost after epoch 1280: 16.701250799823452\n",
            "Cost after epoch 1300: 16.70483929257579\n",
            "Cost after epoch 1320: 16.72369397908193\n",
            "Cost after epoch 1340: 16.726898862856572\n",
            "Cost after epoch 1360: 16.71586227721337\n",
            "Cost after epoch 1380: 16.72869352166104\n",
            "Cost after epoch 1400: 16.734238502622915\n",
            "Cost after epoch 1420: 16.736437716826053\n",
            "Cost after epoch 1440: 16.719300023095208\n",
            "Cost after epoch 1460: 16.742727070113464\n",
            "Cost after epoch 1480: 16.7626850949541\n",
            "Cost after epoch 1500: 16.754777108031025\n",
            "Cost after epoch 1520: 16.72824586986297\n",
            "Cost after epoch 1540: 16.735301741370346\n",
            "Cost after epoch 1560: 16.760945942261866\n",
            "Cost after epoch 1580: 16.7622284127863\n",
            "Cost after epoch 1600: 16.736064607254733\n",
            "Cost after epoch 1620: 16.759922362336454\n",
            "Cost after epoch 1640: 16.76331408906404\n",
            "Cost after epoch 1660: 16.76177830712623\n",
            "Cost after epoch 1680: 16.758571270595866\n",
            "Cost after epoch 1700: 16.763284239341687\n",
            "Cost after epoch 1720: 16.756635588198915\n",
            "Cost after epoch 1740: 16.762163114782403\n",
            "Cost after epoch 1760: 16.735126801131177\n",
            "Cost after epoch 1780: 16.766943240785483\n",
            "Cost after epoch 1800: 16.757578563249275\n",
            "Cost after epoch 1820: 16.76767659174049\n",
            "Cost after epoch 1840: 16.752358436649082\n",
            "Cost after epoch 1860: 16.779716609675695\n",
            "Cost after epoch 1880: 16.765791763824524\n",
            "Cost after epoch 1900: 16.726307189535\n",
            "Cost after epoch 1920: 16.771291356464022\n",
            "Cost after epoch 1940: 16.728925386864116\n",
            "Cost after epoch 1960: 16.76739329638754\n",
            "Cost after epoch 1980: 16.758773039535914\n",
            "Cost after epoch 2000: 16.756441901218537\n",
            "Cost after epoch 2020: 16.740775778163968\n",
            "Cost after epoch 2040: 16.754345038043056\n",
            "Cost after epoch 2060: 16.728236264065092\n",
            "Cost after epoch 2080: 16.743326364387354\n",
            "Cost after epoch 2100: 16.744885000533117\n",
            "Cost after epoch 2120: 16.735380861345583\n",
            "Cost after epoch 2140: 16.696637223536055\n",
            "Cost after epoch 2160: 16.717380589514605\n",
            "Cost after epoch 2180: 16.699592228414325\n",
            "Cost after epoch 2200: 16.728770262025417\n",
            "Cost after epoch 2220: 16.718307558301966\n",
            "Cost after epoch 2240: 16.707940539887325\n",
            "Cost after epoch 2260: 16.70855459135613\n",
            "Cost after epoch 2280: 16.681165005334844\n",
            "Cost after epoch 2300: 16.692151354822574\n",
            "Cost after epoch 2320: 16.689520910265998\n",
            "Cost after epoch 2340: 16.718739458737716\n",
            "Cost after epoch 2360: 16.73575585726979\n",
            "Cost after epoch 2380: 16.7550145148341\n",
            "Cost after epoch 2400: 16.71115720281302\n",
            "Cost after epoch 2420: 16.762670344956916\n",
            "Cost after epoch 2440: 16.753801383399765\n",
            "Cost after epoch 2460: 16.738300349099372\n",
            "Cost after epoch 2480: 16.781983162033136\n",
            "Cost after epoch 2500: 16.78803770484939\n",
            "Cost after epoch 2520: 16.802797617763893\n",
            "Cost after epoch 2540: 16.82587746911684\n",
            "Cost after epoch 2560: 16.81064592335025\n",
            "Cost after epoch 2580: 16.795547743890232\n",
            "Cost after epoch 2600: 16.80305361030218\n",
            "Cost after epoch 2620: 16.837417528311462\n",
            "Cost after epoch 2640: 16.806861103167968\n",
            "Cost after epoch 2660: 16.845381226986255\n",
            "Cost after epoch 2680: 16.830638999513447\n",
            "Cost after epoch 2700: 16.81059251243949\n",
            "Cost after epoch 2720: 16.83610057203023\n",
            "Cost after epoch 2740: 16.83276028326396\n",
            "Cost after epoch 2760: 16.793850196688883\n",
            "Cost after epoch 2780: 16.820655081226995\n",
            "Cost after epoch 2800: 16.803176432242278\n",
            "Cost after epoch 2820: 16.83913329812748\n",
            "Cost after epoch 2840: 16.7690480036414\n",
            "Cost after epoch 2860: 16.826181860803178\n",
            "Cost after epoch 2880: 16.799071461077332\n",
            "Cost after epoch 2900: 16.808178406765524\n",
            "Cost after epoch 2920: 16.798964756325688\n",
            "Cost after epoch 2940: 16.779505301562526\n",
            "Cost after epoch 2960: 16.77047739446651\n",
            "Cost after epoch 2980: 16.771871061205147\n",
            "Cost after epoch 3000: 16.74573858272767\n",
            "Cost after epoch 3020: 16.738536744383417\n",
            "Cost after epoch 3040: 16.720979081943742\n",
            "Cost after epoch 3060: 16.725268934479335\n",
            "Cost after epoch 3080: 16.736807462852923\n",
            "Cost after epoch 3100: 16.726807265127352\n",
            "Cost after epoch 3120: 16.73998555467554\n",
            "Cost after epoch 3140: 16.687428511634902\n",
            "Cost after epoch 3160: 16.711503860110362\n",
            "Cost after epoch 3180: 16.720411004041182\n",
            "Cost after epoch 3200: 16.68693892311477\n",
            "Cost after epoch 3220: 16.67336886503459\n",
            "Cost after epoch 3240: 16.713130744552558\n",
            "Cost after epoch 3260: 16.67098857321163\n",
            "Cost after epoch 3280: 16.704061970192356\n",
            "Cost after epoch 3300: 16.70012062937679\n",
            "Cost after epoch 3320: 16.66962839793196\n",
            "Cost after epoch 3340: 16.696944301679167\n",
            "Cost after epoch 3360: 16.698602603160644\n",
            "Cost after epoch 3380: 16.675629442247637\n",
            "Cost after epoch 3400: 16.708730980502477\n",
            "Cost after epoch 3420: 16.698535077086092\n",
            "Cost after epoch 3440: 16.724458346651804\n",
            "Cost after epoch 3460: 16.691414084761995\n",
            "Cost after epoch 3480: 16.72542252032772\n",
            "Cost after epoch 3500: 16.688341067207375\n",
            "Cost after epoch 3520: 16.732553037880372\n",
            "Cost after epoch 3540: 16.722939270477344\n",
            "Cost after epoch 3560: 16.72672185802271\n",
            "Cost after epoch 3580: 16.754807902246686\n",
            "Cost after epoch 3600: 16.741548634053704\n",
            "Cost after epoch 3620: 16.761562981174052\n",
            "Cost after epoch 3640: 16.731369144901006\n",
            "Cost after epoch 3660: 16.727204513751886\n",
            "Cost after epoch 3680: 16.76097939981616\n",
            "Cost after epoch 3700: 16.736898061434854\n",
            "Cost after epoch 3720: 16.7671325740376\n",
            "Cost after epoch 3740: 16.731994335887244\n",
            "Cost after epoch 3760: 16.733723293436654\n",
            "Cost after epoch 3780: 16.751110787435724\n",
            "Cost after epoch 3800: 16.768098682439152\n",
            "Cost after epoch 3820: 16.756945811032864\n",
            "Cost after epoch 3840: 16.7489702705821\n",
            "Cost after epoch 3860: 16.78689481551028\n",
            "Cost after epoch 3880: 16.780308482916087\n",
            "Cost after epoch 3900: 16.772192980446484\n",
            "Cost after epoch 3920: 16.745515793426335\n",
            "Cost after epoch 3940: 16.733928744077375\n",
            "Cost after epoch 3960: 16.759716921238024\n",
            "Cost after epoch 3980: 16.788305590619043\n",
            "Cost after epoch 4000: 16.734992158684594\n",
            "Cost after epoch 4020: 16.738302011906757\n",
            "Cost after epoch 4040: 16.768455037575276\n",
            "Cost after epoch 4060: 16.766794421761944\n",
            "Cost after epoch 4080: 16.772584580454378\n",
            "Cost after epoch 4100: 16.777422772298845\n",
            "Cost after epoch 4120: 16.739122263155853\n",
            "Cost after epoch 4140: 16.77587762476563\n",
            "Cost after epoch 4160: 16.748603597884674\n",
            "Cost after epoch 4180: 16.756873754210286\n",
            "Cost after epoch 4200: 16.765034262195535\n",
            "Cost after epoch 4220: 16.746914506523236\n",
            "Cost after epoch 4240: 16.769500335064837\n",
            "Cost after epoch 4260: 16.77011502945401\n",
            "Cost after epoch 4280: 16.75928371248078\n",
            "Cost after epoch 4300: 16.748030702614408\n",
            "Cost after epoch 4320: 16.74509903606702\n",
            "Cost after epoch 4340: 16.782867395243606\n",
            "Cost after epoch 4360: 16.779707904491914\n",
            "Cost after epoch 4380: 16.75402854514607\n",
            "Cost after epoch 4400: 16.772838149575414\n",
            "Cost after epoch 4420: 16.741118926995195\n",
            "Cost after epoch 4440: 16.78144092364777\n",
            "Cost after epoch 4460: 16.744168870309196\n",
            "Cost after epoch 4480: 16.79709486431284\n",
            "Cost after epoch 4500: 16.783703043848814\n",
            "Cost after epoch 4520: 16.751766881545496\n",
            "Cost after epoch 4540: 16.741103242990313\n",
            "Cost after epoch 4560: 16.789455212773504\n",
            "Cost after epoch 4580: 16.73485731470611\n",
            "Cost after epoch 4600: 16.776688699322108\n",
            "Cost after epoch 4620: 16.755050646703822\n",
            "Cost after epoch 4640: 16.737177808089413\n",
            "Cost after epoch 4660: 16.78746342185727\n",
            "Cost after epoch 4680: 16.76501833920999\n",
            "Cost after epoch 4700: 16.746707567100856\n",
            "Cost after epoch 4720: 16.744283998382407\n",
            "Cost after epoch 4740: 16.762287907628757\n",
            "Cost after epoch 4760: 16.737989903688614\n",
            "Cost after epoch 4780: 16.739923620667536\n",
            "Cost after epoch 4800: 16.76256090370153\n",
            "Cost after epoch 4820: 16.73890495725514\n",
            "Cost after epoch 4840: 16.760967131263826\n",
            "Cost after epoch 4860: 16.770365126534607\n",
            "Cost after epoch 4880: 16.77360100715942\n",
            "Cost after epoch 4900: 16.74971768118365\n",
            "Cost after epoch 4920: 16.729888375561863\n",
            "Cost after epoch 4940: 16.774615510291564\n",
            "Cost after epoch 4960: 16.74293634782992\n",
            "Cost after epoch 4980: 16.75389649260568\n",
            "Cost after epoch 5000: 16.74775141771708\n",
            "Cost after epoch 5020: 16.749943223699827\n",
            "Cost after epoch 5040: 16.76803044881046\n",
            "Cost after epoch 5060: 16.722294472479263\n",
            "Cost after epoch 5080: 16.763185728403734\n",
            "Cost after epoch 5100: 16.766598667591968\n",
            "Cost after epoch 5120: 16.746697566607192\n",
            "Cost after epoch 5140: 16.719406356425527\n",
            "Cost after epoch 5160: 16.762963710236065\n",
            "Cost after epoch 5180: 16.753602097535246\n",
            "Cost after epoch 5200: 16.772616561846476\n",
            "Cost after epoch 5220: 16.756953996637534\n",
            "Cost after epoch 5240: 16.747576735481307\n",
            "Cost after epoch 5260: 16.727435613275713\n",
            "Cost after epoch 5280: 16.778319164134935\n",
            "Cost after epoch 5300: 16.761068172226455\n",
            "Cost after epoch 5320: 16.72944722630047\n",
            "Cost after epoch 5340: 16.753460866626874\n",
            "Cost after epoch 5360: 16.74569744230371\n",
            "Cost after epoch 5380: 16.730664497741333\n",
            "Cost after epoch 5400: 16.77257734011014\n",
            "Cost after epoch 5420: 16.748295318434696\n",
            "Cost after epoch 5440: 16.74213679480287\n",
            "Cost after epoch 5460: 16.768870728948794\n",
            "Cost after epoch 5480: 16.731467450349626\n",
            "Cost after epoch 5500: 16.78095291768483\n",
            "Cost after epoch 5520: 16.76974747246398\n",
            "Cost after epoch 5540: 16.768775069768232\n",
            "Cost after epoch 5560: 16.759566031662985\n",
            "Cost after epoch 5580: 16.77404593534216\n",
            "Cost after epoch 5600: 16.75878589359644\n",
            "Cost after epoch 5620: 16.753356423189583\n",
            "Cost after epoch 5640: 16.735988123692206\n",
            "Cost after epoch 5660: 16.779815621562545\n",
            "Cost after epoch 5680: 16.780249806799056\n",
            "Cost after epoch 5700: 16.773074006978167\n",
            "Cost after epoch 5720: 16.775330320569342\n",
            "Cost after epoch 5740: 16.73188669921013\n",
            "Cost after epoch 5760: 16.743446993183092\n",
            "Cost after epoch 5780: 16.76742795919151\n",
            "Cost after epoch 5800: 16.76469875532275\n",
            "Cost after epoch 5820: 16.761896217006427\n",
            "Cost after epoch 5840: 16.73371543445399\n",
            "Cost after epoch 5860: 16.71468871631207\n",
            "Cost after epoch 5880: 16.751578503689565\n",
            "Cost after epoch 5900: 16.72497083384717\n",
            "Cost after epoch 5920: 16.75130107395574\n",
            "Cost after epoch 5940: 16.73549754126147\n",
            "Cost after epoch 5960: 16.714930309530697\n",
            "Cost after epoch 5980: 16.71433426071268\n",
            "Cost after epoch 6000: 16.749202024101628\n",
            "Cost after epoch 6020: 16.727282954922217\n",
            "Cost after epoch 6040: 16.75994516334134\n",
            "Cost after epoch 6060: 16.706102522662167\n",
            "Cost after epoch 6080: 16.742936093874384\n",
            "Cost after epoch 6100: 16.745291375980152\n",
            "Cost after epoch 6120: 16.733640297151588\n",
            "Cost after epoch 6140: 16.761058976837806\n",
            "Cost after epoch 6160: 16.74655090615011\n",
            "Cost after epoch 6180: 16.718135021008777\n",
            "Cost after epoch 6200: 16.752420807845436\n",
            "Cost after epoch 6220: 16.725929311429944\n",
            "Cost after epoch 6240: 16.74135625245927\n",
            "Cost after epoch 6260: 16.713162087882022\n",
            "Cost after epoch 6280: 16.726919988737812\n",
            "Cost after epoch 6300: 16.760698499998135\n",
            "Cost after epoch 6320: 16.72044421027776\n",
            "Cost after epoch 6340: 16.752699182397457\n",
            "Cost after epoch 6360: 16.748055880389437\n",
            "Cost after epoch 6380: 16.734609134048476\n",
            "Cost after epoch 6400: 16.75526093949251\n",
            "Cost after epoch 6420: 16.76109680125235\n",
            "Cost after epoch 6440: 16.771852558372657\n",
            "Cost after epoch 6460: 16.764961406890045\n",
            "Cost after epoch 6480: 16.747007129242697\n",
            "Cost after epoch 6500: 16.731160119385116\n",
            "Cost after epoch 6520: 16.735523799210988\n",
            "Cost after epoch 6540: 16.743294169074083\n",
            "Cost after epoch 6560: 16.752407027278426\n",
            "Cost after epoch 6580: 16.76950387103811\n",
            "Cost after epoch 6600: 16.766850857731026\n",
            "Cost after epoch 6620: 16.770172535485145\n",
            "Cost after epoch 6640: 16.74643816303834\n",
            "Cost after epoch 6660: 16.75391854466149\n",
            "Cost after epoch 6680: 16.75087966000636\n",
            "Cost after epoch 6700: 16.741363607746514\n",
            "Cost after epoch 6720: 16.753750899008484\n",
            "Cost after epoch 6740: 16.761718050275185\n",
            "Cost after epoch 6760: 16.76835022880265\n",
            "Cost after epoch 6780: 16.745223934233962\n",
            "Cost after epoch 6800: 16.763008553892888\n",
            "Cost after epoch 6820: 16.7678641837531\n",
            "Cost after epoch 6840: 16.74259940104475\n",
            "Cost after epoch 6860: 16.761207624208655\n",
            "Cost after epoch 6880: 16.753227856649726\n",
            "Cost after epoch 6900: 16.754158557250815\n",
            "Cost after epoch 6920: 16.775870552456112\n",
            "Cost after epoch 6940: 16.755736873478043\n",
            "Cost after epoch 6960: 16.73819184207966\n",
            "Cost after epoch 6980: 16.741284386696876\n",
            "Cost after epoch 7000: 16.765415771186913\n",
            "Cost after epoch 7020: 16.750531503057047\n",
            "Cost after epoch 7040: 16.723352440751793\n",
            "Cost after epoch 7060: 16.725626082347592\n",
            "Cost after epoch 7080: 16.748691936052484\n",
            "Cost after epoch 7100: 16.76219706017631\n",
            "Cost after epoch 7120: 16.759667798045854\n",
            "Cost after epoch 7140: 16.767639377315852\n",
            "Cost after epoch 7160: 16.720407155213014\n",
            "Cost after epoch 7180: 16.75041232584244\n",
            "Cost after epoch 7200: 16.724504284925153\n",
            "Cost after epoch 7220: 16.72223310526833\n",
            "Cost after epoch 7240: 16.715723430998924\n",
            "Cost after epoch 7260: 16.75059622655634\n",
            "Cost after epoch 7280: 16.719786852977535\n",
            "Cost after epoch 7300: 16.745506957756145\n",
            "Cost after epoch 7320: 16.731911933206913\n",
            "Cost after epoch 7340: 16.72461309573236\n",
            "Cost after epoch 7360: 16.7258217635123\n",
            "Cost after epoch 7380: 16.75135266642913\n",
            "Cost after epoch 7400: 16.73087276403825\n",
            "Cost after epoch 7420: 16.741926991529525\n",
            "Cost after epoch 7440: 16.720106495862865\n",
            "Cost after epoch 7460: 16.71869225706456\n",
            "Cost after epoch 7480: 16.713199423035693\n",
            "Cost after epoch 7500: 16.732238340544892\n",
            "Cost after epoch 7520: 16.74131070133705\n",
            "Cost after epoch 7540: 16.71867775581105\n",
            "Cost after epoch 7560: 16.724709161197353\n",
            "Cost after epoch 7580: 16.701619385245582\n",
            "Cost after epoch 7600: 16.71146621521144\n",
            "Cost after epoch 7620: 16.70072637903275\n",
            "Cost after epoch 7640: 16.706230230328707\n",
            "Cost after epoch 7660: 16.703630020368934\n",
            "Cost after epoch 7680: 16.727784056938017\n",
            "Cost after epoch 7700: 16.703919038179578\n",
            "Cost after epoch 7720: 16.69423175631462\n",
            "Cost after epoch 7740: 16.721953572678853\n",
            "Cost after epoch 7760: 16.69126957082973\n",
            "Cost after epoch 7780: 16.70280842887518\n",
            "Cost after epoch 7800: 16.712490559010707\n",
            "Cost after epoch 7820: 16.722174716651043\n",
            "Cost after epoch 7840: 16.72794819979663\n",
            "Cost after epoch 7860: 16.72512113276859\n",
            "Cost after epoch 7880: 16.715724878370015\n",
            "Cost after epoch 7900: 16.705793053251277\n",
            "Cost after epoch 7920: 16.71085550294307\n",
            "Cost after epoch 7940: 16.723484815814334\n",
            "Cost after epoch 7960: 16.71376932732131\n",
            "Cost after epoch 7980: 16.696735130129557\n",
            "Cost after epoch 8000: 16.714168847736808\n",
            "Cost after epoch 8020: 16.70984927510445\n",
            "Cost after epoch 8040: 16.696736357194045\n",
            "Cost after epoch 8060: 16.705074534258372\n",
            "Cost after epoch 8080: 16.706035693204928\n",
            "Cost after epoch 8100: 16.72291742162608\n",
            "Cost after epoch 8120: 16.707376966593607\n",
            "Cost after epoch 8140: 16.7087815262479\n",
            "Cost after epoch 8160: 16.699827438797456\n",
            "Cost after epoch 8180: 16.68298738087535\n",
            "Cost after epoch 8200: 16.707900308728785\n",
            "Cost after epoch 8220: 16.67725900693067\n",
            "Cost after epoch 8240: 16.692895504989558\n",
            "Cost after epoch 8260: 16.70733123796436\n",
            "Cost after epoch 8280: 16.7054201393244\n",
            "Cost after epoch 8300: 16.677592789132408\n",
            "Cost after epoch 8320: 16.690664747508002\n",
            "Cost after epoch 8340: 16.69320849053972\n",
            "Cost after epoch 8360: 16.705468962512782\n",
            "Cost after epoch 8380: 16.7053803654607\n",
            "Cost after epoch 8400: 16.696829428157077\n",
            "Cost after epoch 8420: 16.68927495892435\n",
            "Cost after epoch 8440: 16.687265086540087\n",
            "Cost after epoch 8460: 16.681076283432247\n",
            "Cost after epoch 8480: 16.693581842182866\n",
            "Cost after epoch 8500: 16.689375073378223\n",
            "Cost after epoch 8520: 16.68975613963726\n",
            "Cost after epoch 8540: 16.665380758264135\n",
            "Cost after epoch 8560: 16.67980058641904\n",
            "Cost after epoch 8580: 16.680242455076826\n",
            "Cost after epoch 8600: 16.674178437381904\n",
            "Cost after epoch 8620: 16.68186416631488\n",
            "Cost after epoch 8640: 16.681670800140008\n",
            "Cost after epoch 8660: 16.683719563405415\n",
            "Cost after epoch 8680: 16.666874233505858\n",
            "Cost after epoch 8700: 16.66542198836696\n",
            "Cost after epoch 8720: 16.660912662405984\n",
            "Cost after epoch 8740: 16.673144194430932\n",
            "Cost after epoch 8760: 16.68219856955809\n",
            "Cost after epoch 8780: 16.654512500651236\n",
            "Cost after epoch 8800: 16.68550007810479\n",
            "Cost after epoch 8820: 16.66215456998399\n",
            "Cost after epoch 8840: 16.650975946682102\n",
            "Cost after epoch 8860: 16.659784762590817\n",
            "Cost after epoch 8880: 16.66936661025318\n",
            "Cost after epoch 8900: 16.6668093912936\n",
            "Cost after epoch 8920: 16.671154108791093\n",
            "Cost after epoch 8940: 16.66789286460177\n",
            "Cost after epoch 8960: 16.66949750486981\n",
            "Cost after epoch 8980: 16.651471176568347\n",
            "Cost after epoch 9000: 16.654047664096947\n",
            "Cost after epoch 9020: 16.664228767891636\n",
            "Cost after epoch 9040: 16.65574863444391\n",
            "Cost after epoch 9060: 16.6534295841294\n",
            "Cost after epoch 9080: 16.65576836846072\n",
            "Cost after epoch 9100: 16.660279129642316\n",
            "Cost after epoch 9120: 16.668249892907436\n",
            "Cost after epoch 9140: 16.664391349856125\n",
            "Cost after epoch 9160: 16.648859891922672\n",
            "Cost after epoch 9180: 16.660989081779384\n",
            "Cost after epoch 9200: 16.665631447103745\n",
            "Cost after epoch 9220: 16.63478988580682\n",
            "Cost after epoch 9240: 16.650302003834724\n",
            "Cost after epoch 9260: 16.650114819269533\n",
            "Cost after epoch 9280: 16.657734114187353\n",
            "Cost after epoch 9300: 16.651732393581536\n",
            "Cost after epoch 9320: 16.645389684120346\n",
            "Cost after epoch 9340: 16.656926127825024\n",
            "Cost after epoch 9360: 16.666081240217633\n",
            "Cost after epoch 9380: 16.66500526854277\n",
            "Cost after epoch 9400: 16.64168384367754\n",
            "Cost after epoch 9420: 16.637708177680043\n",
            "Cost after epoch 9440: 16.63526759806943\n",
            "Cost after epoch 9460: 16.63851523441451\n",
            "Cost after epoch 9480: 16.645097960363202\n",
            "Cost after epoch 9500: 16.656126178349265\n",
            "Cost after epoch 9520: 16.633627027444867\n",
            "Cost after epoch 9540: 16.63435158426566\n",
            "Cost after epoch 9560: 16.646749354797024\n",
            "Cost after epoch 9580: 16.655241721971457\n",
            "Cost after epoch 9600: 16.642659388082507\n",
            "Cost after epoch 9620: 16.64431804006642\n",
            "Cost after epoch 9640: 16.643710671028188\n",
            "Cost after epoch 9660: 16.64375211897941\n",
            "Cost after epoch 9680: 16.648561050781666\n",
            "Cost after epoch 9700: 16.647998268240833\n",
            "Cost after epoch 9720: 16.63274215298197\n",
            "Cost after epoch 9740: 16.65304083278697\n",
            "Cost after epoch 9760: 16.661424387182528\n",
            "Cost after epoch 9780: 16.653534547696676\n",
            "Cost after epoch 9800: 16.64681842949988\n",
            "Cost after epoch 9820: 16.6357021203671\n",
            "Cost after epoch 9840: 16.6557794951352\n",
            "Cost after epoch 9860: 16.633526722773546\n",
            "Cost after epoch 9880: 16.653128878603557\n",
            "Cost after epoch 9900: 16.654761377694765\n",
            "Cost after epoch 9920: 16.635343898536917\n",
            "Cost after epoch 9940: 16.64707605287544\n",
            "Cost after epoch 9960: 16.658862625686815\n",
            "Cost after epoch 9980: 16.6405442873733\n",
            "training time: 0:01:33.073617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5zowXVw8VKT"
      },
      "source": [
        "X_test = np.arange(vocab_size)\n",
        "X_test = np.expand_dims(X_test, axis=0)\n",
        "softmax_test, _ = forward_propagation(X_test, paras)\n",
        "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:, :]\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSn34oS-nM0",
        "outputId": "de8cd580-812b-4a34-97e1-6fd23fd035b6"
      },
      "source": [
        "for input_ind in range(vocab_size):\n",
        "    input_word = id_to_word[input_ind]\n",
        "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
        "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this's neighbor words: ['input', 'us', 'case', 'cbow']\n",
            "just's neighbor words: ['but', 'embedding', 'model', 'lines']\n",
            "two's neighbor words: ['put', 'words', 'three', 'word']\n",
            "called's neighbor words: ['of', 'word', 'we', 'weights']\n",
            "which's neighbor words: ['required', 'nuerons', 'are', 'hidden']\n",
            "input's neighbor words: ['good', 'with', 'given', 'sequence']\n",
            "continuous's neighbor words: ['words', 'it', 'bag', 'called']\n",
            "lines's neighbor words: ['a', 'which', 'of', 'the']\n",
            "in's neighbor words: ['model', 'the', 'skip', 'this']\n",
            "embeddings's neighbor words: ['particular', 'are', 'required', 'of']\n",
            "other's neighbor words: ['word2vec', 'variant', 'one', 'vector']\n",
            "of's neighbor words: ['word', 'the', 'a', 'input']\n",
            "cbow's neighbor words: ['helps', 'or', 'variant', 'this']\n",
            "good's neighbor words: ['accuracy', 'input', 'with', 'sequence']\n",
            "nuerons's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "is's neighbor words: ['one', 'predict', 'bag', 'it']\n",
            "it's neighbor words: ['continuous', 'inputs', 'called', 'is']\n",
            "next's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "converges's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "a's neighbor words: ['the', 'approach', 'since', 'input']\n",
            "works's neighbor words: ['same', 'these', 'word2vec', 'on']\n",
            "once's neighbor words: ['converges', 'skip', 'gram', 'model']\n",
            "get's neighbor words: ['of', 'word', 'we', 'not']\n",
            "on's neighbor words: ['lines', 'works', 'these', 'word2vec']\n",
            "predict's neighbor words: ['vector', 'word', 'probability', 'is']\n",
            "not's neighbor words: ['hot', 'one', 'is', 'embedding']\n",
            "or's neighbor words: ['variant', 'bag', 'cbow', 'this']\n",
            "these's neighbor words: ['but', 'embedding', 'model', 'one']\n",
            "trained's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "consider's neighbor words: ['of', 'word', 'we', 'not']\n",
            "same's neighbor words: ['with', 'sequence', 'skip', 'as']\n",
            "embedding's neighbor words: ['just', 'hot', 'one', 'a']\n",
            "vector's neighbor words: ['one', 'in', 'as', 'probability']\n",
            "words's neighbor words: ['this', 'i', 'together', 'but']\n",
            "i's neighbor words: ['sequence', 'together', 'words', 'e']\n",
            "model's neighbor words: ['once', 'these', 'on', 'model']\n",
            "one's neighbor words: ['two', 'vector', 'just', 'but']\n",
            "sequence's neighbor words: ['accuracy', 'good', 'given', 'with']\n",
            "hidden's neighbor words: ['which', 'nuerons', 'weights', 'layer']\n",
            "us's neighbor words: ['next', 'this', 'helps', 'predict']\n",
            "with's neighbor words: ['accuracy', 'good', 'given', 'sequence']\n",
            "skip's neighbor words: ['once', 'gram', 'as', 'in']\n",
            "we's neighbor words: ['sequence', 'weights', 'since', 'get']\n",
            "helps's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "given's neighbor words: ['with', 'sequence', 'word', 'of']\n",
            "put's neighbor words: ['e', 'two', 'three', 'i']\n",
            "case's neighbor words: ['layer', 'approach', 'this', 'nuerons']\n",
            "bag's neighbor words: ['or', 'continuous', 'called', 'words']\n",
            "word's neighbor words: ['of', 'given', 'three', 'consider']\n",
            "probability's neighbor words: ['as', 'to', 'vector', 'but']\n",
            "gram's neighbor words: ['the', 'predict', 'us', 'once']\n",
            "approach's neighbor words: ['case', 'different', 'slightly', 'this']\n",
            "e's neighbor words: ['then', 'put', 'i', 'together']\n",
            "inputs's neighbor words: ['called', 'it', 'a', 'input']\n",
            "are's neighbor words: ['embeddings', 'required', 'layer', 'which']\n",
            "but's neighbor words: ['words', 'slightly', 'three', 'two']\n",
            "slightly's neighbor words: ['in', 'approach', 'different', 'but']\n",
            "word2vec's neighbor words: ['on', 'works', 'other', 'these']\n",
            "layer's neighbor words: ['the', 'are', 'a', 'which']\n",
            "weights's neighbor words: ['hidden', 'get', 'we', 'nuerons']\n",
            "required's neighbor words: ['a', 'which', 'of', 'the']\n",
            "particular's neighbor words: ['we', 'since', 'embeddings', 'vector']\n",
            "accuracy's neighbor words: ['good', 'sequence', 'with', 'of']\n",
            "together's neighbor words: ['a', 'which', 'the', 'of']\n",
            "as's neighbor words: ['gram', 'skip', 'probability', 'in']\n",
            "different's neighbor words: ['this', 'slightly', 'approach', 'in']\n",
            "since's neighbor words: ['a', 'which', 'of', 'the']\n",
            "to's neighbor words: ['probability', 'trained', 'predict', 'model']\n",
            "the's neighbor words: ['sequence', 'of', 'input', 'is']\n",
            "hot's neighbor words: ['of', 'word', 'we', 'not']\n",
            "then's neighbor words: ['is', 'e', 'this', 'called']\n",
            "variant's neighbor words: ['model', 'predict', 'us', 'word2vec']\n",
            "three's neighbor words: ['together', 'two', 'put', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-Bp9pQ_IXg",
        "outputId": "9e2466de-a86c-4f36-ab92-e386000cf5df"
      },
      "source": [
        "paras['WRD_EMB']"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.89209732, -3.11677676, -0.79012119, ...,  1.6040923 ,\n",
              "         3.10709667,  0.81803109],\n",
              "       [ 1.88919442, 13.35062229,  1.02518092, ...,  7.07621041,\n",
              "        -3.56491074, -4.66378038],\n",
              "       [ 2.1793213 , 11.69738518,  5.90534222, ...,  3.16409806,\n",
              "        -0.5337679 , -6.73368607],\n",
              "       ...,\n",
              "       [-4.04082846,  0.81663646, -1.59273427, ..., -4.10346159,\n",
              "         1.74121803,  0.65262142],\n",
              "       [-7.06803813,  4.04347912,  5.36364787, ..., -8.38799681,\n",
              "        -0.95461679,  0.53521974],\n",
              "       [ 2.07126895,  6.63753624,  4.18762649, ..., -3.3023751 ,\n",
              "        -1.73041244, -2.69823607]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXwL6yyWdsBp"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7P7EB9Utily",
        "outputId": "7153c7dc-622d-41b3-9b64-fae495b8575e"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "common_texts"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "CQFQ0l6wtxbt",
        "outputId": "99316f81-c16c-4422-fab8-3d417129aba6"
      },
      "source": [
        "model = Word2Vec(sentences=common_texts)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-ba4e36324421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZlxpNjOuBJ8",
        "outputId": "4b3d6da5-4a97-44c2-8d3c-2dd981ad1ce8"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzz5QfEjuI6w"
      },
      "source": [
        "docVec = [nltk.word_tokenize(doc)]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBOqAOVuvFS4",
        "outputId": "6119ac6c-1b1e-46cc-d1fc-bd1e00d3d3af"
      },
      "source": [
        "docVec"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The',\n",
              "  'other',\n",
              "  'variant',\n",
              "  'of',\n",
              "  'Word2Vec',\n",
              "  'model',\n",
              "  'works',\n",
              "  'on',\n",
              "  'these',\n",
              "  'same',\n",
              "  'lines',\n",
              "  'but',\n",
              "  'with',\n",
              "  'a',\n",
              "  'slightly',\n",
              "  'different',\n",
              "  'approach',\n",
              "  '.',\n",
              "  'In',\n",
              "  'this',\n",
              "  'case',\n",
              "  ',',\n",
              "  'the',\n",
              "  'input',\n",
              "  'layer',\n",
              "  'is',\n",
              "  'not',\n",
              "  'a',\n",
              "  'one',\n",
              "  'hot',\n",
              "  'embedding',\n",
              "  'vector',\n",
              "  'of',\n",
              "  'just',\n",
              "  'one',\n",
              "  'word',\n",
              "  'but',\n",
              "  'two-three',\n",
              "  'words',\n",
              "  'put',\n",
              "  'together',\n",
              "  'i.e',\n",
              "  '.',\n",
              "  'a',\n",
              "  'sequence',\n",
              "  '.',\n",
              "  'Then',\n",
              "  'the',\n",
              "  'model',\n",
              "  'is',\n",
              "  'trained',\n",
              "  'to',\n",
              "  'predict',\n",
              "  'the',\n",
              "  'probability',\n",
              "  'vector',\n",
              "  ',',\n",
              "  'same',\n",
              "  'as',\n",
              "  'in',\n",
              "  'skip-gram',\n",
              "  'model',\n",
              "  '.',\n",
              "  'Once',\n",
              "  'the',\n",
              "  'model',\n",
              "  'converges',\n",
              "  ',',\n",
              "  'we',\n",
              "  'get',\n",
              "  'the',\n",
              "  'weights',\n",
              "  'of',\n",
              "  'the',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'nuerons',\n",
              "  'which',\n",
              "  'are',\n",
              "  'the',\n",
              "  'required',\n",
              "  'embeddings',\n",
              "  'of',\n",
              "  'a',\n",
              "  'particular',\n",
              "  'word',\n",
              "  '.',\n",
              "  'Since',\n",
              "  ',',\n",
              "  'we',\n",
              "  'consider',\n",
              "  'a',\n",
              "  'sequence',\n",
              "  'of',\n",
              "  'inputs',\n",
              "  ',',\n",
              "  'it',\n",
              "  'is',\n",
              "  'called',\n",
              "  '‘',\n",
              "  'continuous',\n",
              "  'bag',\n",
              "  'of',\n",
              "  'words',\n",
              "  'or',\n",
              "  'CBOW',\n",
              "  '’',\n",
              "  '.',\n",
              "  'This',\n",
              "  'variant',\n",
              "  'helps',\n",
              "  'us',\n",
              "  'predict',\n",
              "  'the',\n",
              "  'next',\n",
              "  'word',\n",
              "  'of',\n",
              "  'the',\n",
              "  'given',\n",
              "  'input',\n",
              "  'sequence',\n",
              "  'with',\n",
              "  'good',\n",
              "  'accuracy',\n",
              "  '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgIgczVZvGO0"
      },
      "source": [
        "model = Word2Vec(docVec, min_count=2)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJsLgRWwvPGN",
        "outputId": "75a043d9-fe01-447e-c385-5affa85502d5"
      },
      "source": [
        "model.most_similar('word')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('variant', 0.16292086243629456),\n",
              " ('but', 0.09921394288539886),\n",
              " ('layer', 0.09630829095840454),\n",
              " ('with', 0.08683191239833832),\n",
              " ('model', 0.05424391105771065),\n",
              " (',', 0.04354755952954292),\n",
              " ('of', 0.021269526332616806),\n",
              " ('predict', 0.015442294999957085),\n",
              " ('is', 0.014847943559288979),\n",
              " ('sequence', 0.00835694745182991)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlDPNlYAvTEn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}