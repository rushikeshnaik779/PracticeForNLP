concept of attention for information reterival 
IT has three mechanism keys, queries, values 
has encoder and decoder hidden state 
key and value comes from encoder hidden states 
while queries come from decoder hidden states
attention is flexible enough to find the words which are not in alignment 
MEANING : 
1. ATTENTION is an addded layer that lets a model focus on what's important
2. Q, K, V are used for information retrieval inside the attention layer 
3. The flexible system finds matches even between languages with very different gramatical structures. 


